{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an efficient supervised word translator\n",
    "\n",
    "Based on : \"Exploiting Similarities among Languages for Machine Translation\" of Tomas Mikolov, Quoc V. Le & Ilya Sutskever (2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load function for pretrained versions of word embeddings\n",
    "def load_embeddings(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_path = '/Users/louismonier/Downloads/Monolingual/wiki.en.vec' \n",
    "fr_path = '/Users/louismonier/Downloads/Monolingual/wiki.fr.vec'\n",
    "nmax = 50000  # maximum number of word embeddings to load\n",
    "\n",
    "# load monolingual word embeddings \n",
    "src_embeddings, src_id2word, src_word2id = load_embeddings(fr_path, nmax) # source = french \n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_embeddings(eng_path, nmax) # target = english"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FRENCH to ENGLISH translation\n",
    "\n",
    "* source language = french\n",
    "* target language = english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ground-truth bilingual dictionaries function\n",
    "def load_dic(path):\n",
    "    dico_full = {}\n",
    "    vectors_src=[]\n",
    "    vectors_tgt = []\n",
    "    with io.open(path,'r',encoding='utf_8') as f:\n",
    "        for i,line in enumerate(f):\n",
    "            word_src, word_tgt = line.rstrip().split(' ',1)\n",
    "            if word_tgt in tgt_word2id :\n",
    "                dico_full[word_src]=word_tgt\n",
    "    for key in dico_full.keys() :\n",
    "            vectors_src.append(src_embeddings[src_word2id[key]])\n",
    "            vectors_tgt.append(tgt_embeddings[tgt_word2id[dico_full[key]]])\n",
    "    X = np.vstack(vectors_src)\n",
    "    Z = np.vstack (vectors_tgt)\n",
    "    return dico_full,X,Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & test bilingual dictionaries\n",
    "path_train = r'/Users/louismonier/Downloads/Monolingual/fr-en.0-5000.txt' \n",
    "path_test = r'/Users/louismonier/Downloads/Monolingual/fr-en.5000-6500.txt'\n",
    "dico_train, X_train, Z_train = load_dic(path_train)\n",
    "dico_test, X_test, Z_test = load_dic(path_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I - Build a learning a linear mapping from a source (french) to a target (english) embedding space thanks to a translation Matrix W \n",
    "\n",
    "Suppose we are given a set of word pairs and their associated vector representations ${ \\{x_i , z_i\\} }^{n}_{i=1}$ , where $x_i$ ∈ $R^{d1}$ is the distributed representation of word i in the source language, and $z_i$ ∈ $R^{d2}$ is the vector representation of its translation.\n",
    "\n",
    "It is our goal to find a transformation matrix W such that W xi approximates $z_i$ . In practice, W can be learned by the following optimization problem :\n",
    "\n",
    "$$ \\underset{W}{min} C (W) = \\underset{W}{min} \\sum_{i=1}^{n} \\| W x_i - z_i \\|^2 $$ \n",
    "\n",
    "which we solve with gradient descent (GD), stochastic gradient descent (SGD) or mini-batch gradient descent (BGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_train.shape[0], \"training samples\")\n",
    "print(X_test.shape[0], \"test samples\")\n",
    "\n",
    "print(\"d1 dimension :\", X_train.shape[1])\n",
    "print(\"d2 dimension :\", X_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to minimize \n",
    "# normalisation added\n",
    "def C(W,X,Z):\n",
    "    S = 0\n",
    "    S = (1/(X.shape[0])) * sum(np.linalg.norm(np.dot(X, W.T) - Z, axis=1)**2)\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient of the function to minimize\n",
    "# normalisation added\n",
    "def GradW(W,X,Z):\n",
    "    G = np.zeros((Z_train.shape[1], X_train.shape[1]))\n",
    "    G = (2/(X.shape[0])) * np.dot(X.T, (np.dot(X, W.T) - Z)).T\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GD function\n",
    "def GradientDescent(eta, niter): \n",
    "    W = np.random.rand(300,300) # random initialisation of W\n",
    "    value_C = np.zeros(niter)\n",
    "    for t in range(niter): \n",
    "        value_C[t] = C(W,X_train,Z_train)\n",
    "        W -= eta*GradW(W,X_train,Z_train)\n",
    "    print(\"Done...\")\n",
    "    return (W, value_C) #,acc_test,acc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGC or BGD\n",
    "def StochasticGradientDescent(eta, niter, nb):\n",
    "    W = np.random.rand(300,300) # random initialisation of W\n",
    "    grad = np.zeros((Z_train.shape[1], X_train.shape[1]))\n",
    "    value_C = np.zeros(niter)\n",
    "    \n",
    "    for t in range(niter):\n",
    "        if nb>1 : # BGD\n",
    "            l = np.random.choice(len(dico_train), nb) # size of batch\n",
    "            for p in l : # ok\n",
    "                grad += (2*np.outer((np.dot(W, X_train[p]) - Z_train[p]), X_train[p]))\n",
    "        else : # SGD\n",
    "            l = np.random.randint(low=0,high=len(dico_train)) \n",
    "            grad += (2*np.outer((np.dot(W, X_train[l]) - Z_train[l]), X_train[l]))\n",
    "        \n",
    "        grad = (1/nb)*grad\n",
    "        W -= eta*grad\n",
    "        value_C[t] = C(W,X_train,Z_train)\n",
    "    print(\"Done...\")\n",
    "    return (W, value_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "The goal is to find the best W to map the source language to the target one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eta = 5e-5\n",
    "niter = 250\n",
    "\n",
    "W_GD, C_GD = GradientDescent(eta, niter) # GD\n",
    "W_SGD, C_SGD = StochasticGradientDescent(eta, niter, 1) # SGD\n",
    "W_BGD, C_BGD = StochasticGradientDescent(eta, niter, 10) # BGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(niter), C_GD, label=\"Gradient Descent\")\n",
    "plt.plot(range(niter), C_SGD, label=\"Stochastic Gradient Descent\")\n",
    "plt.plot(range(niter), C_BGD, label=\"Batch Gradient Descent\")\n",
    "plt.ylabel('Cost Function')\n",
    "plt.xlabel('Iteration')\n",
    "plt.title(\"Cost function\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WTF pour le SGD et le BGD ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fastest training\n",
    "Wmin = np.dot(np.linalg.pinv(X_train), Z_train).T "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the prediction time, for any given new word and its continuous vector representation x, we can map it to the other language space by computing $z = Wx$. Then, we find the word whose representation is closest to z in the target language space, using cosine similarity as the distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(W, new_word, k=5):\n",
    "    x = src_embeddings[src_word2id[new_word]] # vector representation of new_word in the source space\n",
    "    z = np.dot(W, x) # vector representation of the translated word in the target space\n",
    "\n",
    "    # representation closest to z in the target language space, using cosine similarity as the distance metric\n",
    "    z_pred1 = np.argmax(sklearn.metrics.pairwise.cosine_similarity(z.reshape(1,300),tgt_embeddings))\n",
    "\n",
    "    # top k closest word embeddings in the target space\n",
    "    z_predk = sklearn.metrics.pairwise.cosine_similarity(z.reshape(1,300),tgt_embeddings)[0].argsort()[-k:][::-1]\n",
    "    \n",
    "    return [tgt_id2word[z_pred1]], [tgt_id2word[z_predk[i]] for i in range(len(z_predk))]  # return the id of the translated word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a litte test\n",
    "top1, top5 = prediction(W_GD, \"bateau\", k=5)\n",
    "print(\"Top 1 translation for 'bateau' is :\", top1)\n",
    "print(\"Top 5 translations for 'bateau' are :\", top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a translation French to English dictionary\n",
    "def prediction_dict(dico, W, kk):\n",
    "    dico_pred1 = {}\n",
    "    dico_predk = {}\n",
    "    i = 0\n",
    "    for word in dico.keys() :\n",
    "        if (i%100==0):\n",
    "            print(\"Progress :\", round(i/len(dico_test)*100,1), \"%\")\n",
    "        dico_pred1[word], dico_predk[word] = prediction(W, word, k) # lists\n",
    "        i += 1\n",
    "    print(\"Done...\")\n",
    "    return dico_pred1, dico_predk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test predictions for W from GD\n",
    "\n",
    "# k-top accuracy (\"acc@k\")\n",
    "k = 5\n",
    "\n",
    "dico_pred1, dico_predk = prediction_dict(dico_test, W_GD, k) \n",
    "dico_pred1_best, dico_predk_best = prediction_dict(dico_test, Wmin, k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure of the accuracy of the dictionnary\n",
    "# output is a list\n",
    "def accuracy(dpred1, dpredk, dico):\n",
    "    acc1 = [0]\n",
    "    acck = [0]\n",
    "    \n",
    "    for key in dico.keys():\n",
    "        add1, addk = 0, 0\n",
    "        \n",
    "        if dico[key] == dpred1[key][0]:\n",
    "            add1 = 1\n",
    "        acc1.append(acc1[-1] + add1)  \n",
    "    \n",
    "        for i in np.arange(k):\n",
    "            if dico[key] == dpredk[key][i]:\n",
    "                addk = 1\n",
    "                break   \n",
    "        acck.append(acck[-1] + addk) \n",
    "    \n",
    "    acc1 = [i/len(dico) for i in acc1]   \n",
    "    acck = [i/len(dico) for i in acck]\n",
    "    \n",
    "    return acc1, acck # nb de mots bien prédits/nb de mots total     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure of the accuracy of the dictionnary\n",
    "# output is just final result\n",
    "def fast_accuracy(dpred1, dpredk, dico):\n",
    "    acc1 = 0\n",
    "    acck = 0\n",
    "    for key in dico.keys():     \n",
    "        if dico[key] == dpred1[key][0]:\n",
    "            acc1 += 1\n",
    "        \n",
    "        for i in np.arange(k):\n",
    "            if dico[key] == dpredk[key][i]:\n",
    "                acck += 1\n",
    "                break   \n",
    "                \n",
    "    acc1 /= len(dico)\n",
    "    acck /= len(dico)\n",
    "    \n",
    "    return acc1, acck # nb de mots bien prédits/nb de mots total     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1_test, acck_test = accuracy(dico_pred1, dico_predk, dico_test)\n",
    "acc1_test_best, acck_test_best = accuracy(dico_pred1_best, dico_predk_best, dico_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(acc1_test, label=\"Acc_1\")\n",
    "plt.plot(acck_test,label=\"Acc_k\")\n",
    "plt.plot([i/len(dico_test) for i in range(len(dico_test))],label=\"identity\",alpha=0.4)\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Iteration')\n",
    "plt.title(\"Accuracy test set (Gradient Descent method)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Gradient descent method :\")\n",
    "print(\"Final accuracy @1 =\", round(acc1_test[-1]*100, 2), \"%\")\n",
    "print(\"Final accuracy @5 =\", round(acck_test[-1]*100, 2), \"%\")\n",
    "print(\"\")\n",
    "print(\"Analytical method :\")\n",
    "print(\"Final accuracy @1 =\", round(acc1_test_best[-1]*100, 2), \"%\")\n",
    "print(\"Final accuracy @5 =\", round(acck_test_best[-1]*100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annex part I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter sensitivity\n",
    "eta_list = np.array([0.001, 0.003, 0.01, 0.03, 0.09, 0.3]) \n",
    "niter = 100\n",
    "\n",
    "plt.clf\n",
    "\n",
    "for ieta in np.arange(0,eta_list.size):\n",
    "    eta = eta_list[ieta]\n",
    "    W_GD, C_GD = GradientDescent(eta, niter)\n",
    "    plt.plot(C_GD, label='$\\eta$ = %s' % eta_list[ieta])\n",
    "    \n",
    "plt.ylabel('Cost Function')\n",
    "plt.xlabel('Iteration')\n",
    "plt.title(\"Tuning learning rate $\\eta$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "plot_embeddings = np.asfarray(X_test[:128,:], dtype='float')\n",
    "low_dim_embs = tsne.fit_transform(plot_embeddings)\n",
    "#labels = [dico_train[i] for i in range(len(dico_train))]\n",
    "plt.scatter(low_dim_embs[:,0], low_dim_embs[:,1], marker='*')#, labels)\n",
    "\n",
    "\n",
    "plt.title('Visualization of the multilingual word embedding space')\n",
    "\n",
    "#for key in list(dico_train.keys()):\n",
    " #   print(key)\n",
    "  #  plt.annotate(list(dico_train.keys())[key], xy=(low_dim_embs[127,0], low_dim_embs[127,1]), \n",
    "   #          xytext=(0, 0), textcoords='offset points', fontsize=10,color='red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II - Adressing the inconsistency\n",
    "\n",
    "Based on : \n",
    "* \"Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation\" of Chao Xing, Dong Wang, Chao Liu & Yiye Lin (2015)\n",
    "\n",
    "(Xing et al. (2015) argued that Mikolov’s linear matrix should be orthogonal, and introduced an\n",
    "approximate procedure composed of gradient descent updates and repeated applications of the SVD)\n",
    "\n",
    "* \"Offline bilingual word vectors, orthogonal transformations and the inverted softmax\" of Samuel L. Smith, David H. P. Turban, Steven Hamblin & Nils Y. Hammerla\n",
    "\n",
    "\n",
    "**Intuition :**\n",
    "A self-consistent linear transformation between vector\n",
    "spaces should be orthogonal. Intuitively, the transformation is a rotation, and it is found using the\n",
    "singular value decomposition (SVD). The SVD aligns the translations remarkably well.\n",
    "\n",
    "\n",
    "\n",
    "If $W$ maps the source language into the target, then $W^T$ maps the target language back into the source. Then, the transformation $W$ should be an orthogonal matrix $O$ satisfying $O^T O = Id$\n",
    "\n",
    "\n",
    "So, we still try to minimize the following optimisation problem :\n",
    "$$ \\underset{W}{min} \\sum_{i=1}^{n} \\| W x_i - z_i \\|^2 $$ \n",
    "\n",
    "Except, this time $W$ should be also an orthogonal matrix $O$. The problem can be rewritten as finding the maximum of : \n",
    "\n",
    "$$ \\underset{O}{max} \\sum_{i=1}^{n} z_i^T O x_i $$ \n",
    "With the orthogonal constraint ($Id$ is the identity matrix) : $$ O^T O = Id $$\n",
    "\n",
    "The cost function is minimize by : \n",
    "$$ O = U V^T $$ \n",
    "Where $U$ and $V$ are obtained from SVD of $Y^T X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "U, Sg, V = np.linalg.svd(np.dot(Z_train.T, X_train), full_matrices=True)\n",
    "O = np.dot(U, V.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a litte test\n",
    "top1, top5 = prediction(Wmin, \"arbre\", k=5)\n",
    "print(\"Top 1 translation for 'bateau' is :\", top1)\n",
    "print(\"Top 5 translations for 'bateau' are :\", top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = C(Wmin,X_train,Z_train)\n",
    "print(A)\n",
    "B = C(O,X_train,Z_train)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
    "    \"\"\"\n",
    "    Source and target dictionaries are the FastVector objects of\n",
    "    source/target languages. bilingual_dictionary is a list of \n",
    "    translation pair tuples [(source_word, target_word), ...].\n",
    "    \"\"\"\n",
    "    source_matrix = []\n",
    "    target_matrix = []\n",
    "\n",
    "    for (source, target) in bilingual_dictionary:\n",
    "        if source in source_dictionary and target in target_dictionary:\n",
    "            source_matrix.append(source_dictionary[source])\n",
    "            target_matrix.append(target_dictionary[target])\n",
    "\n",
    "    # return training matrices\n",
    "    return np.array(source_matrix), np.array(target_matrix)\n",
    "\n",
    "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
    "    \"\"\"\n",
    "    Source and target matrices are numpy arrays, shape\n",
    "    (dictionary_length, embedding_dimension). These contain paired\n",
    "    word vectors from the bilingual dictionary.\n",
    "    \"\"\"\n",
    "    # optionally normalize the training vectors\n",
    "    if normalize_vectors:\n",
    "        source_matrix = normalized(source_matrix)\n",
    "        target_matrix = normalized(target_matrix)\n",
    "\n",
    "    # perform the SVD\n",
    "    product = np.matmul(source_matrix.transpose(), target_matrix)\n",
    "    U, s, V = np.linalg.svd(product)\n",
    "\n",
    "    # return orthogonal transformation which aligns source language to the target\n",
    "    return np.matmul(U, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dico_pred1_o, dico_predk_o = prediction_dict(dico_test, O, k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1_test_o, acck_test_o = accuracy(dico_pred1_o, dico_predk_o, dico_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gradient descent method :\")\n",
    "print(\"Final accuracy @1 =\", round(acc1_test[-1]*100, 2), \"%\")\n",
    "print(\"Final accuracy @5 =\", round(acck_test[-1]*100, 2), \"%\")\n",
    "print(\"\")\n",
    "print(\"Orthogonal method :\")\n",
    "print(\"Final accuracy @1 =\", round(acc1_test_o[-1]*100, 2), \"%\")\n",
    "print(\"Final accuracy @5 =\", round(acck_test_o[-1]*100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverted softmax (needed ?)\n",
    "Sm = np.dot(Z_train, np.dot(U, np.dot(V.T, X_train.T))) # similarity matrix\n",
    "beta = 1.0 # how to maximize the log ? \n",
    "Ps = np.exp(beta * Sm)\n",
    "Ps /= np.sum(ps, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefinition of the optimization task\n",
    "def C_ortho(W,X,Z):\n",
    "    S = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        S += np.dot((np.dot(W, X[i]).T), Z[i])\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient of function to minimize\n",
    "def GradW_ortho(W,X,Z):\n",
    "    G = 0\n",
    "    G = np.dot(X.T, Z)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orthogonal GD function\n",
    "def GradientDescent_ortho(alpha, niter)\n",
    "    W = np.random.rand(300,300) # random initialisation of W\n",
    "    C_ortho = np.zeros(niter)\n",
    "    \n",
    "    for t in range(niter):\n",
    "        \n",
    "        C_ortho[t] = C_ortho(W,X_train,Z_train)\n",
    "        W += alpha*GradW_ortho(W,X_train,Z_train)\n",
    "         \n",
    "    #rajouter contrainte d'orthogonalité sur W \n",
    "\n",
    "    return(W, C_ortho)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
