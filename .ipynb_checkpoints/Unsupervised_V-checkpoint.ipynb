{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "from statistics import stdev\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd.variable import Variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load function for pretrained versions of word embeddings\n",
    "def load_embeddings(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eng_path = '/Users/louismonier/Downloads/Monolingual/wiki.en.vec' \n",
    "#fr_path = '/Users/louismonier/Downloads/Monolingual/wiki.fr.vec'\n",
    "\n",
    "eng_path = '/Users/vince/DataProjetNLP/wiki.en.vec'\n",
    "fr_path = '/Users/vince/DataProjetNLP/wiki.fr.vec'\n",
    "\n",
    "\n",
    "nmax = 50000  # maximum number of word embeddings to load\n",
    "\n",
    "# load monolingual word embeddings \n",
    "src_embeddings, src_id2word, src_word2id = load_embeddings(fr_path, nmax) # source = french \n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_embeddings(eng_path, nmax) # target = english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ground-truth bilingual dictionaries function\n",
    "def load_dic(path):\n",
    "    dico_full = {}\n",
    "    vectors_src=[]\n",
    "    vectors_tgt = []\n",
    "    with io.open(path,'r',encoding='utf_8') as f:\n",
    "        for i,line in enumerate(f):\n",
    "            word_src, word_tgt = line.rstrip().split(' ',1)\n",
    "            if word_tgt in tgt_word2id :\n",
    "                dico_full[word_src]=word_tgt\n",
    "    for key in dico_full.keys() :\n",
    "            vectors_src.append(src_embeddings[src_word2id[key]])\n",
    "            vectors_tgt.append(tgt_embeddings[tgt_word2id[dico_full[key]]])\n",
    "    X = np.vstack(vectors_src)\n",
    "    Z = np.vstack (vectors_tgt)\n",
    "    return dico_full,X,Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# train & test bilingual dictionaries\n",
    "path_train = '/Users/vince/DataProjetNLP/fr-en.0-5000.txt' \n",
    "path_test = '/Users/vince/DataProjetNLP/fr-en.5000-6500.txt'\n",
    "dico_train, X_train, Z_train = load_dic(path_train)\n",
    "dico_test, X_test, Z_test = load_dic(path_test) \n",
    "print(type(X_train[0]))\n",
    "X_train, Z_train, X_test, Z_test = map(torch.tensor, (X_train, Z_train, X_test, Z_test)) #TRANSFORM INTO TORCH TENSORS\n",
    "print(type(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to minimize \n",
    "# normalisation added\n",
    "def C(W,X,Z):\n",
    "    S = 0\n",
    "    S = sum(np.linalg.norm(np.dot(X, W.T) - Z, axis=1)**2) \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4971 training samples\n",
      "1483 test samples\n",
      "Vectors Dimension : 300\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[0], \"training samples\")\n",
    "print(X_test.shape[0], \"test samples\")\n",
    "dim = X_train.shape[1]\n",
    "print(\"Vectors Dimension :\", dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4971, 300])\n",
      "torch.Size([300, 4971])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "y = X_train.view(X_train.shape[1],-1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX_train = torch.rand(1,300,10)\n",
    "testX_test = torch.rand(1,300,10)\n",
    "testX_train = testX_train.view((10,300))\n",
    "testX_test = testX_test.view((10,300))\n",
    "testY_test = torch.as_tensor([0,1,1,0,1,0,1,0,1,0],dtype = torch.float).view(-1,1)\n",
    "testY_train = torch.as_tensor([0,0,1,0,1,0,0,1,1,1],dtype = torch.float).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,d):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.h1 = nn.Linear(dim,2048,bias=True)\n",
    "        self.h2 = nn.Linear(2048,2048,bias=True)\n",
    "        self.out = nn.Linear(2048,1,bias=True)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = x.view(x.shape[0],-1) #Reshape \n",
    "        y = F.relu(self.h1(y))\n",
    "        y = F.relu(self.h2(y))\n",
    "        y = self.out(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (h1): Linear(in_features=300, out_features=2048, bias=True)\n",
      "  (h2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (out): Linear(in_features=2048, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(Discriminator(testX_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0335],\n",
       "        [ 0.0647],\n",
       "        [ 0.0696],\n",
       "        [ 0.0376],\n",
       "        [ 0.0509],\n",
       "        [ 0.0293],\n",
       "        [ 0.0388],\n",
       "        [-0.0084],\n",
       "        [ 0.0494],\n",
       "        [ 0.0951]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc = Discriminator(testX_test.shape)\n",
    "disc.forward(testX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (h1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "  (h2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  (out): Linear(in_features=2048, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mapping(nn.Module):\n",
    "    def __init__(self,d):\n",
    "        super(Mapping,self).__init__()\n",
    "        self.l1 = nn.Linear(300,300)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.linear1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LossDisc(y, y_pred):\n",
    "    return(-math.log((y_pred**y)*(1-y_pred)**(1-y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.605170185988091"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LossDisc(1,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.605170185988091"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def LossMap(y,y_pred):\n",
    "    return(-math.log((y_pred**(1-y))*(1-y_pred)**y))\n",
    "LossMap(1,0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrim = Discriminator(300)\n",
    "gen = Mapping(300)\n",
    "optimizer = torch.optim.SGD(discrim.parameters(),lr = 0.1)\n",
    "criterion = LossDisc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = Discriminator(testX_test.shape)\n",
    "map = Mapping(testX_test.shape)\n",
    "for epoch in range(3): #3 Epochs \n",
    "    for iteration in range(30):\n",
    "        if iteration % 10 == 0 :\n",
    "            print(\"epoch = %d, iteration = %d\"%(epoch,iteration))\n",
    "        # discriminator trained 3 times for every mapping training\n",
    "        for i in range(3):\n",
    "            disc.train()\n",
    "            #Set gradient to zero before computation at every step\n",
    "            optimizer.zero_grad()\n",
    "            src_lang_word_id = torch.Tensor(32).random_(50000).long()\n",
    "            src_lang_word_emb = src_embedding_learnable(src_lang_word_id).cuda()\n",
    "            target_lang_word_id = torch.Tensor(32).random_(50000).long()\n",
    "            target_lang_word_emb = target_embedding_learnable(target_lang_word_id).cuda()\n",
    "            src_mult_mapper = mapper(src_lang_word_emb).cuda()\n",
    "            input_tensor = torch.cat([src_mult_mapper,target_lang_word_emb],0).cuda()\n",
    "            output_tensor = torch.Tensor(64).zero_().float().cuda()\n",
    "            output_tensor[:32] = 1 -0.2 #Smoothing\n",
    "            output_tensor[32:] = 0.2\n",
    "            prediction = discriminator(input_tensor).cuda()\n",
    "            #Compute loss and propogate backward\n",
    "            loss = criterion1(prediction,output_tensor).cuda()\n",
    "            loss.backward()\n",
    "            optimizer1.step()\n",
    "\n",
    "        # mapping training \n",
    "        discriminator.eval()\n",
    "        #Set gradient to zero before computation at every step\n",
    "        optimizer2.zero_grad()\n",
    "        src_lang_word_id = torch.Tensor(32).random_(50000).long()\n",
    "        src_lang_word_emb = src_embedding_learnable(src_lang_word_id).cuda()\n",
    "        target_lang_word_id = torch.Tensor(32).random_(50000).long()\n",
    "        target_lang_word_emb = target_embedding_learnable(target_lang_word_id).cuda()\n",
    "        src_mult_mapper = mapper(src_lang_word_emb)\n",
    "        input_tensor = torch.cat([src_mult_mapper,target_lang_word_emb],0).cuda()\n",
    "        output_tensor = torch.Tensor(64).zero_().float().cuda()\n",
    "        output_tensor[:32] = 1 -0.2 #Smoothing\n",
    "        output_tensor[32:] = 0.2\n",
    "        prediction = discriminator(input_tensor).cuda()\n",
    "        loss = criterion2(prediction,1-output_tensor).cuda()\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "        mapping_tensor = mapper.linear1.weight.data\n",
    "        mapping_tensor.copy_((1.01) * mapping_tensor - 0.01 * mapping_tensor.mm(mapping_tensor.t().mm(mapping_tensor)))\n",
    "\n",
    "        \n",
    "    #Validation through proxy parralel dictionary construction (both directions) and CSLS\n",
    "    src_emb_map_validation = mapper(src_embedding_learnable.weight.cuda()).cuda()\n",
    "    target_emb_map_validation = target_embedding_learnable.weight.cuda()\n",
    "    src_emb_map_validation = src_emb_map_validation/src_emb_map_validation.norm(2, 1, keepdim=True).expand_as(src_emb_map_validation)\n",
    "    target_emb_map_validation = target_emb_map_validation/target_emb_map_validation.norm(2, 1, keepdim=True).expand_as(target_emb_map_validation)\n",
    "    src_to_target_dictionary = top_words(src_emb_map_validation,target_emb_map_validation)\n",
    "    target_to_src_dictionary = top_words(target_emb_map_validation,src_emb_map_validation)\n",
    "    dictionary = proxy_construct_dictionary(src_emb_map_validation,target_emb_map_validation,src_to_target_dictionary,target_to_src_dictionary)\n",
    "    if dictionary is None:\n",
    "        mean_cosine = -1e9\n",
    "    else:\n",
    "        mean_cosine = (src_emb_map_validation[dictionary[:, 0]] * target_emb_map_validation[dictionary[:, 1]]).sum(1).mean()\n",
    "\n",
    "    # Dampenining by 0.95\n",
    "    optimizer1.param_groups[0]['lr'] = 0.95*optimizer1.param_groups[0]['lr']\n",
    "    optimizer2.param_groups[0]['lr'] = 0.95*optimizer2.param_groups[0]['lr']\n",
    "    #Divide by 2 if validation decreases\n",
    "    if mean_cosine > 0 and mean_cosine < validation_tracker :\n",
    "        optimizer1.param_groups[0]['lr'] = 0.5*optimizer1.param_groups[0]['lr']\n",
    "        optimizer2.param_groups[0]['lr'] = 0.5*optimizer2.param_groups[0]['lr']\n",
    "        validation_tracker = max(mean_cosine,validation_tracker)\n",
    "    print(epoch,mean_cosine)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (h1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "  (h2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  (out): Linear(in_features=2048, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "\n",
    "for n in range(n_epochs):\n",
    "    i = 0\n",
    "    for i in range lenx, y in train_loader:\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        i+=1\n",
    "        if i % 100 ==0 :\n",
    "            print(\"Epoch [{}/{}], [{}/{}], batch loss = {}\".format(n+1,n_epochs,i*bs,n_data_train,loss))      \n",
    "            \n",
    "    acc = 0    \n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            y_pred = model(x)\n",
    "            y_pred = torch.argmax(y_pred,dim=1)\n",
    "            acc += (y_pred == y).sum().item()\n",
    "        acc = acc/n_data_test\n",
    "    print(\"Epoch [{}/{}], test accuracy = {}\".format(n+1,n_epochs,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
