{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_path = '/Users/vince/DataProjetNLP/wiki.multi.en.vec.txt'\n",
    "fr_path = '/Users/vince/DataProjetNLP/wiki.multi.fr.vec.txt'\n",
    "nmax = 50000  # maximum number of word embeddings to load\n",
    "\n",
    "src_embeddings, src_id2word, src_word2id = load_embeddings(eng_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_embeddings(fr_path, nmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(x,y):\n",
    "    return(dot(x, y)/(norm(x)*norm(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn(word, src_emb, src_id2word, tgt_emb, tgt_id2word, K=5):\n",
    "    print(\"Nearest neighbors of \\\"%s\\\":\" % word)\n",
    "    word2id = {v: k for k, v in src_id2word.items()}\n",
    "    word_emb = src_emb[word2id[word]]\n",
    "    scores = (tgt_emb / np.linalg.norm(tgt_emb, 2, 1)[:, None]).dot(word_emb / np.linalg.norm(word_emb))\n",
    "    #sorted_scores = scores.argsort()\n",
    "    #qwertz = np.sort(scores)\n",
    "    k_best = scores.argsort()[-K:][::-1]\n",
    "    #print(word_emb)\n",
    "    #print(sorted_scores[:10])\n",
    "    #print(qwertz[49990:])\n",
    "    for i, idx in enumerate(k_best):\n",
    "        print('%.4f - %s' % (scores[idx], tgt_id2word[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-929fa33471ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[0mpath_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/Users/vince/DataProjetNLP/wiki.multi.fr.vec.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[0mdico_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m \u001b[0mdico_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-929fa33471ea>\u001b[0m in \u001b[0;36mload_dic\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mvectors_src\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_embeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msrc_word2id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mvectors_tgt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt_embeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtgt_word2id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdico_full\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors_src\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvectors_tgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdico_full\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \"\"\"\n\u001b[0;32m    282\u001b[0m     \u001b[0m_warn_for_nonsequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Nov 11 13:31:38 2019\n",
    "\n",
    "@author: Vince\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import io\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#word embeddings : anglais et français\n",
    "\n",
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)                                                                                                                    \n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id\n",
    "\n",
    "tgt_path = '/Users/vince/DataProjetNLP/wiki.multi.en.vec.txt'\n",
    "src_path = '/Users/vince/DataProjetNLP/wiki.multi.fr.vec.txt'\n",
    "nmax = 50000  # maximum number of word embeddings to load\n",
    "\n",
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)\n",
    "\n",
    "\n",
    "#données : train et test\n",
    "\n",
    "def load_dic(path):\n",
    "    dico_full = {}\n",
    "    vectors_src=[]\n",
    "    vectors_tgt = []\n",
    "    with io.open(path,'r',encoding='utf_8') as f:\n",
    "        for i,line in enumerate(f):\n",
    "            word_src, word_tgt = line.rstrip().split(' ',1)\n",
    "            if word_tgt in tgt_word2id :\n",
    "                dico_full[word_src]=word_tgt\n",
    "    for key in dico_full.keys() :\n",
    "            vectors_src.append(src_embeddings[src_word2id[key]])\n",
    "            vectors_tgt.append(tgt_embeddings[tgt_word2id[dico_full[key]]])\n",
    "    X = np.vstack(vectors_src)\n",
    "    Z = np.vstack (vectors_tgt)\n",
    "    return dico_full,X,Z\n",
    "\n",
    "path_train = '/Users/vince/DataProjetNLP/fr-en.0-5000.txt'\n",
    "path_test = '/Users/vince/DataProjetNLP/wiki.multi.fr.vec.txt'\n",
    "dico_train, X_train, Z_train = load_dic(path_train)\n",
    "dico_test, X_test, Z_test = load_dic(path_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LINEAR TRANSFORM : Translation Matrix W \n",
    "\n",
    "def C(W,X,Z):\n",
    "    S=0\n",
    "    for i in range(X.shape[0]):\n",
    "        S=S+np.linalg.norm(np.dot(W,X[i])-Z[i])**2\n",
    "    return S\n",
    "\n",
    "def dC_dW(W,X,Z):\n",
    "    S=0\n",
    "    for i in range(X.shape[0]):\n",
    "        S=S+2*np.outer((np.dot(W,X[i])-Z[i]),X[i])\n",
    "    return S\n",
    "\n",
    "#W = np.random.rand(300,300)\n",
    "W = np.eye(300)\n",
    "eta = 0.001\n",
    "delta = 0.01\n",
    "N = 300\n",
    "nb = 1000\n",
    "\n",
    "#reprendre code theo \n",
    "\n",
    "#descente de gradient  \n",
    "#def gradientDescent(eta,):\n",
    "valeur_C = []\n",
    "for t in range(N):\n",
    "    print(t)\n",
    "    tmp_W = W \n",
    "    W = tmp_W - eta*dC_dW(tmp_W,X_train,Z_train)\n",
    "    valeur_C.append(C(W,X_train,Z_train))\n",
    "print(valeur_C)\n",
    "print(dC_dW(W,X_train,Z_train))\n",
    "print(np.linalg.norm(dC_dW(W,X_train,Z_train)))\n",
    "\n",
    "#ou descente de gradient stochastique\n",
    "\n",
    "norm_grad = []\n",
    "for t in range(N):\n",
    "    print(t)\n",
    "    l = np.random.randint(low=0,high=len(dico_train)) \n",
    "    tmp_W = W \n",
    "    W = W - eta*dC_dW(tmp_W,X_train,Z_train)\n",
    "    valeur_C.append(C(W,X_train,Z_train))\n",
    "print(valeur_C)\n",
    "print(dC_dW(W,X_train,Z_train))\n",
    "print(np.linalg.norm(dC_dW(W,X_train,Z_train)))\n",
    "\n",
    "\n",
    "#ORTHOGONAL TRANSFORM \n",
    "\n",
    "\n",
    "#validation : sur dico_test, X_test et Z_test\n",
    "        \n",
    "def prediction(W,mot):\n",
    "    x = src_embeddings[src_word2id[mot]]\n",
    "    z = np.dot(W,x)\n",
    "    z_pred = np.argmax(sklearn.metrics.pairwise.cosine_similarity(z.reshape(1,300),tgt_embeddings)) #celui qui a la plus grande similarite cos avec z\n",
    "    return tgt_id2word[z_pred]\n",
    "\n",
    "dico_pred={}\n",
    "i=0\n",
    "for mot in dico_test.keys() :\n",
    "    print(i)\n",
    "    dico_pred[mot]=prediction(W,mot)\n",
    "    i+=1\n",
    "\n",
    "def accuracy(dico_pred,dico_test):\n",
    "    c=0\n",
    "    for key in dico_test.keys():\n",
    "        if dico_test[key] == dico_pred[key]:\n",
    "            c+=1\n",
    "    return(c/len(dico_test)) #nb de mots bien prédits/nb de mots total\n",
    "    \n",
    "accuracy_test = accuracy(dico_pred,dico_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
