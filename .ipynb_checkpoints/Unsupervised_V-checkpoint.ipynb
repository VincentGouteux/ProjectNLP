{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an efficient unsupervised word translator\n",
    "\n",
    "Based on : \"Word Translation Without Parallel Data\" by Alexis Conneau, Guillaume Lample, Marc Aurelio Ranzato, Ludovic Denoyer & Hervé Jégou (2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "#from torch.autograd.variable import Variable\n",
    "\n",
    "from scipy.stats import special_ortho_group\n",
    "\n",
    "#from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load function for pretrained versions of word embeddings\n",
    "def load_embeddings(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "#eng_path = '/Users/louismonier/Downloads/Monolingual/wiki.en.vec' \n",
    "#fr_path = '/Users/louismonier/Downloads/Monolingual/wiki.fr.vec'\n",
    "##########\n",
    "eng_path = '/Users/vince/DataProjetNLP/wiki.en.vec'\n",
    "fr_path = '/Users/vince/DataProjetNLP/wiki.fr.vec'\n",
    "##########\n",
    "nmax = 50000  # maximum number of word embeddings to load\n",
    "\n",
    "# load monolingual word embeddings \n",
    "src_embeddings, src_id2word, src_word2id = load_embeddings(fr_path, nmax) # source = french \n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_embeddings(eng_path, nmax) # target = english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ground-truth bilingual dictionaries function\n",
    "def load_dic(path):\n",
    "    dico_full = {}\n",
    "    vectors_src=[]\n",
    "    vectors_tgt = []\n",
    "    with io.open(path,'r',encoding='utf_8') as f:\n",
    "        for i,line in enumerate(f):\n",
    "            word_src, word_tgt = line.rstrip().split(' ',1)\n",
    "            if word_tgt in tgt_word2id :\n",
    "                dico_full[word_src]=word_tgt\n",
    "    for key in dico_full.keys() :\n",
    "            vectors_src.append(src_embeddings[src_word2id[key]])\n",
    "            vectors_tgt.append(tgt_embeddings[tgt_word2id[dico_full[key]]])\n",
    "    X = np.vstack(vectors_src)\n",
    "    Z = np.vstack (vectors_tgt)\n",
    "    return dico_full,X,Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# train & test bilingual dictionaries\n",
    "##########\n",
    "#path_train = r'/Users/louismonier/Downloads/Monolingual/fr-en.0-5000.txt' \n",
    "#path_test = r'/Users/louismonier/Downloads/Monolingual/fr-en.5000-6500.txt'\n",
    "##########\n",
    "path_train = '/Users/vince/DataProjetNLP/fr-en.0-5000.txt' \n",
    "path_test = '/Users/vince/DataProjetNLP/fr-en.5000-6500.txt'\n",
    "##########\n",
    "\n",
    "dico_train, X_train, Z_train = load_dic(path_train)\n",
    "dico_test, X_test, Z_test = load_dic(path_test)\n",
    "\n",
    "# convert embeddings vectors into torch tensors \n",
    "print(type(X_train[0]))\n",
    "X_train, Z_train, X_test, Z_test = map(torch.tensor, (X_train, Z_train, X_test, Z_test)) \n",
    "print(type(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4971 training samples\n",
      "1483 test samples\n",
      "Vectors dimension : 300\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[0], \"training samples\")\n",
    "print(X_test.shape[0], \"test samples\")\n",
    "dim = X_train.shape[1]\n",
    "print(\"Vectors dimension :\", dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the discriminator \n",
    "\n",
    "Recall what is the objective of the discriminator here : ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.h1 = nn.Linear(dim, 2048,bias=True) # 1st hidden layer\n",
    "        self.h2 = nn.Linear(2048,2048,bias=True) # 2nd hidden layer\n",
    "        self.out = nn.Linear(2048,1,bias=True) # output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.dropout(x, p = 0.1) # dropout pour ajouter du bruit\n",
    "        x = F.leaky_relu(self.h1(x), negative_slope=0.2)\n",
    "        x = F.leaky_relu(self.h2(x), negative_slope=0.2)\n",
    "        y = torch.sigmoid(self.out(x)) # ouput = proba\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def LossDisc(y, y_pred):\n",
    "#    return(-math.log((y_pred**y)*(1-y_pred)**(1-y)))\n",
    "\n",
    "# or : \n",
    "LossD = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the generator \n",
    "\n",
    "Recall what is the objective of the discriminator here : ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple linear function \n",
    "# can be seen at a neural network whose weights are elements of W \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.l1 = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = self.l1(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could put this inside the class...\n",
    "\n",
    "# to ensure that the matrix stays close to the manifold of orthogonal matrices after each update\n",
    "def ortho_update(W, beta):\n",
    "    W = (1+beta)*W - beta*torch.mm(torch.mm(W, W.t()), W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def LossGen(y, y_pred):\n",
    "#    return(-math.log((y_pred**(1-y))*(1-y_pred)**y))\n",
    "\n",
    "# or :\n",
    "LossG = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's time to train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrim = Discriminator(dim)\n",
    "gen = Generator(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimD = optim.SGD(discrim.parameters(), lr=0.1)\n",
    "optimG = optim.SGD(gen.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1, iteration = 0\n",
      "0\n",
      "epoch = 1, iteration = 10\n",
      "epoch = 1, iteration = 20\n",
      "epoch = 1, iteration = 30\n",
      "epoch = 1, iteration = 40\n",
      "epoch = 1, iteration = 50\n",
      "50\n",
      "epoch = 1, iteration = 60\n",
      "epoch = 1, iteration = 70\n",
      "epoch = 1, iteration = 80\n",
      "epoch = 1, iteration = 90\n",
      "epoch = 1, iteration = 100\n",
      "100\n",
      "epoch = 1, iteration = 110\n",
      "epoch = 1, iteration = 120\n",
      "epoch = 1, iteration = 130\n",
      "epoch = 1, iteration = 140\n",
      "epoch = 1, iteration = 150\n",
      "150\n",
      "epoch = 1, iteration = 160\n",
      "epoch = 1, iteration = 170\n",
      "epoch = 1, iteration = 180\n",
      "epoch = 1, iteration = 190\n",
      "epoch = 1, iteration = 200\n",
      "200\n",
      "epoch = 1, iteration = 210\n",
      "epoch = 1, iteration = 220\n",
      "epoch = 1, iteration = 230\n",
      "epoch = 1, iteration = 240\n"
     ]
    }
   ],
   "source": [
    "#for epoch in range(3): #3 Epochs \n",
    "W_train = []\n",
    "N = 32 # nb of \"fake\" tgt // Wx\n",
    "M = 32 # nb of \"true\" tgt // y\n",
    "\n",
    "W = special_ortho_group.rvs(dim) # initialisation of W in SO(300)\n",
    "W = torch.tensor(W,dtype=torch.float) # conversion to tensor\n",
    "beta = 0.01\n",
    "\n",
    "# ini the weights of the discrim\n",
    "for p in discrim.parameters():\n",
    "    p.data = torch.zeros(p.data.shape)\n",
    "\n",
    "niter = 250\n",
    "for iteration in range(niter):\n",
    "    if iteration % 10 == 0 :\n",
    "        print(\"epoch = %d, iteration = %d\"%(1,iteration))\n",
    "    \n",
    "    # DISCRIMINATOR TRAINING\n",
    "    for i in range(3):\n",
    "        # set the discrimintator training mode to True\n",
    "        discrim.train()\n",
    "        \n",
    "        # set descrim gradient to zero before computation at every step\n",
    "        optimD.zero_grad()\n",
    "        \n",
    "        ## create a new batch of N fake & M true data to train the discriminator\n",
    "        # generate 32 random words from the source \n",
    "        rand_src_word_id = torch.Tensor(N).random_(nmax).long()\n",
    "        src_word_emb = src_embeddings[rand_src_word_id.numpy()]\n",
    "        src_word_emb = torch.tensor(src_word_emb, dtype=torch.float) # conversion to tensor\n",
    "    \n",
    "        wsrc_gen = gen(src_word_emb) # translated words (Wx)\n",
    "    \n",
    "        # generate 32 random words from the target\n",
    "        rand_tgt_word_id = torch.Tensor(M).random_(nmax).long()\n",
    "        tgt_word_emb = tgt_embeddings[rand_tgt_word_id.numpy()]\n",
    "        tgt_word_emb = torch.tensor(tgt_word_emb, dtype=torch.float) # conversion to tensor\n",
    "\n",
    "        # concatenation of Wx and y aka traductions and tgt words\n",
    "        input_tensor = torch.cat([wsrc_gen, tgt_word_emb],0)\n",
    "\n",
    "        # output tensor is the answer the discriminator should give\n",
    "        output_tensor = torch.Tensor(64).zero_().float()\n",
    "        # we can smooth the answer by creating thresholds (# 0.8 # smoothing 80% # smoothing 20%)\n",
    "        output_tensor[:N] = 1 # discrim should predict 100% proba of belonging to the src \n",
    "        output_tensor[N:] = 0 # discrim should predict 100% proba of belonging to the tgt \n",
    "        \n",
    "        # prediction of the discriminator\n",
    "        prediction = discrim(input_tensor)\n",
    "        \n",
    "        # compute loss & propogate backward\n",
    "        loss_discrim = LossD(prediction, output_tensor)\n",
    "        # loss_discrim = LossDisc(output_tensor, prediction)\n",
    "        \n",
    "        loss_discrim.backward()\n",
    "        optimD.step()\n",
    "\n",
    "    # GENERATOR TRAINING\n",
    "    \n",
    "    # set the discrimintator training mode to False \n",
    "    discrim.eval()\n",
    "    \n",
    "    # set descrim gradient to zero before computation at every step\n",
    "    optimG.zero_grad()\n",
    "    \n",
    "    ## create a new batch of N fake & M true data to train the generator\n",
    "    # generate 32 random words from the source \n",
    "    rand_src_word_id = torch.Tensor(N).random_(nmax).long()\n",
    "    src_word_emb = src_embeddings[rand_src_word_id.numpy()]\n",
    "    src_word_emb = torch.tensor(src_word_emb, dtype=torch.float) # conversion to tensor\n",
    "\n",
    "    wsrc_gen = gen(src_word_emb) # translated words (Wx)\n",
    "\n",
    "    # generate 32 random words from the target\n",
    "    rand_tgt_word_id = torch.Tensor(M).random_(nmax).long()\n",
    "    tgt_word_emb = tgt_embeddings[rand_tgt_word_id.numpy()]\n",
    "    tgt_word_emb = torch.tensor(tgt_word_emb, dtype=torch.float) # conversion to tensor    \n",
    "    \n",
    "    # concatenation of Wx and y aka traductions and tgt words\n",
    "    input_tensor = torch.cat([wsrc_gen, tgt_word_emb],0)\n",
    "    \n",
    "    # output tensor is the answer the discriminator should give\n",
    "    output_tensor = torch.Tensor(64).zero_().float()\n",
    "    # we can smooth the answer by creating thresholds (# 0.8 # smoothing 80% # smoothing 20%)\n",
    "    output_tensor[:N] = 1 # discrim should predict 100% proba of belonging to the src \n",
    "    output_tensor[N:] = 0 # discrim should predict 100% proba of belonging to the tgt \n",
    "      \n",
    "    # prediction of the discriminator on the new batch\n",
    "    prediction = discrim(input_tensor)\n",
    "\n",
    "    # compute loss & propogate backward\n",
    "    # the discriminator is fooled if he predicts the contrary of what he should have predicted\n",
    "    loss_gen = LossG(prediction, 1-output_tensor)\n",
    "    # loss_gen = LossGen(1-output_tensor, prediction)\n",
    "\n",
    "    loss_gen.backward()\n",
    "    optimG.step()    \n",
    "\n",
    "    W_trained = gen.l1.weight.data # get the weights of the generator which are the elements of W\n",
    "    W_ortho = ortho_update(W_trained, beta)\n",
    "    if iteration%50 ==0 :\n",
    "        W_train.append(W_trained)\n",
    "        print(iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(W_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 possibles functions of accuracy = \n",
    "# - Test if we traduced well the word = sum Indicatrice(Wxi!=zi) (en gros nb de mots mal traduits) # en fait non c'est con on a pas les traductions\n",
    "# - Test how far we are from the \"supposed\" translation\n",
    "# compute Wxi find the closest z (SUPPOSED TO BE ITS TRANSLATION) and cumpute eculidian distance\n",
    "\n",
    "rand_test_word_id = torch.Tensor(30).random_(nmax).long()\n",
    "X_test = src_embeddings[rand_test_word_id.numpy()] # GARDER LES MEMES POUR LES TESTS\n",
    "\n",
    "\n",
    "\n",
    "def test_accuracy(X_text,W_trained):\n",
    "    loss = 0\n",
    "    for x in X_test : #get all french words\n",
    "        word2id = {v: k for k, v in src_id2word.items()}\n",
    "        word_emb_new = np.dot(W_trained,x)\n",
    "        scores = (tgt_embeddings / np.linalg.norm(tgt_embeddings, 2, 1)[:, None]).dot(word_emb_new / np.linalg.norm(word_emb_new))\n",
    "        best = scores.argsort()[-1:][::-1]\n",
    "        nearest_eng_emb = src_embeddings[best]\n",
    "        loss = loss + np.linalg.norm(word_emb_new-nearest_eng_emb)  \n",
    "    return(loss)\n",
    "    \n",
    "#Pous savoir si le model est pertinent tester l'accuracy pour W_trained a epoch 1,5,10\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 300)\n"
     ]
    }
   ],
   "source": [
    "W = np.eye(300)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198.72726599187766"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(X_test,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171.31996669882994"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(X_test,W_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171.31996669882994"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(X_test,W_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165.9447194630705\n",
      "165.9447194630705\n",
      "165.9447194630705\n",
      "165.9447194630705\n",
      "165.9447194630705\n"
     ]
    }
   ],
   "source": [
    "for x in W_train : \n",
    "    print(test_accuracy(X_test,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0559, -0.0108, -0.0072,  ..., -0.0314, -0.0140,  0.0070],\n",
      "        [-0.0421,  0.0025, -0.0103,  ..., -0.0390,  0.0356,  0.0541],\n",
      "        [ 0.0531, -0.0565,  0.0060,  ..., -0.0151,  0.0012,  0.0488],\n",
      "        ...,\n",
      "        [ 0.0139, -0.0178, -0.0433,  ...,  0.0128,  0.0170,  0.0182],\n",
      "        [-0.0320, -0.0556,  0.0203,  ..., -0.0137, -0.0246,  0.0561],\n",
      "        [-0.0299,  0.0014, -0.0106,  ...,  0.0323,  0.0395,  0.0443]])\n",
      "tensor([[-0.0559, -0.0108, -0.0072,  ..., -0.0314, -0.0140,  0.0070],\n",
      "        [-0.0421,  0.0025, -0.0103,  ..., -0.0390,  0.0356,  0.0541],\n",
      "        [ 0.0531, -0.0565,  0.0060,  ..., -0.0151,  0.0012,  0.0488],\n",
      "        ...,\n",
      "        [ 0.0139, -0.0178, -0.0433,  ...,  0.0128,  0.0170,  0.0182],\n",
      "        [-0.0320, -0.0556,  0.0203,  ..., -0.0137, -0.0246,  0.0561],\n",
      "        [-0.0299,  0.0014, -0.0106,  ...,  0.0323,  0.0395,  0.0443]])\n",
      "tensor([[-0.0559, -0.0108, -0.0072,  ..., -0.0314, -0.0140,  0.0070],\n",
      "        [-0.0421,  0.0025, -0.0103,  ..., -0.0390,  0.0356,  0.0541],\n",
      "        [ 0.0531, -0.0565,  0.0060,  ..., -0.0151,  0.0012,  0.0488],\n",
      "        ...,\n",
      "        [ 0.0139, -0.0178, -0.0433,  ...,  0.0128,  0.0170,  0.0182],\n",
      "        [-0.0320, -0.0556,  0.0203,  ..., -0.0137, -0.0246,  0.0561],\n",
      "        [-0.0299,  0.0014, -0.0106,  ...,  0.0323,  0.0395,  0.0443]])\n",
      "tensor([[-0.0559, -0.0108, -0.0072,  ..., -0.0314, -0.0140,  0.0070],\n",
      "        [-0.0421,  0.0025, -0.0103,  ..., -0.0390,  0.0356,  0.0541],\n",
      "        [ 0.0531, -0.0565,  0.0060,  ..., -0.0151,  0.0012,  0.0488],\n",
      "        ...,\n",
      "        [ 0.0139, -0.0178, -0.0433,  ...,  0.0128,  0.0170,  0.0182],\n",
      "        [-0.0320, -0.0556,  0.0203,  ..., -0.0137, -0.0246,  0.0561],\n",
      "        [-0.0299,  0.0014, -0.0106,  ...,  0.0323,  0.0395,  0.0443]])\n",
      "tensor([[-0.0559, -0.0108, -0.0072,  ..., -0.0314, -0.0140,  0.0070],\n",
      "        [-0.0421,  0.0025, -0.0103,  ..., -0.0390,  0.0356,  0.0541],\n",
      "        [ 0.0531, -0.0565,  0.0060,  ..., -0.0151,  0.0012,  0.0488],\n",
      "        ...,\n",
      "        [ 0.0139, -0.0178, -0.0433,  ...,  0.0128,  0.0170,  0.0182],\n",
      "        [-0.0320, -0.0556,  0.0203,  ..., -0.0137, -0.0246,  0.0561],\n",
      "        [-0.0299,  0.0014, -0.0106,  ...,  0.0323,  0.0395,  0.0443]])\n"
     ]
    }
   ],
   "source": [
    "for x in W_train :\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
