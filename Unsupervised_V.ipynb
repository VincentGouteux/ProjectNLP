{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "from statistics import stdev\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "from torch.autograd.variable import Variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load function for pretrained versions of word embeddings\n",
    "def load_embeddings(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eng_path = '/Users/louismonier/Downloads/Monolingual/wiki.en.vec' \n",
    "#fr_path = '/Users/louismonier/Downloads/Monolingual/wiki.fr.vec'\n",
    "\n",
    "eng_path = '/Users/vince/DataProjetNLP/wiki.en.vec'\n",
    "fr_path = '/Users/vince/DataProjetNLP/wiki.fr.vec'\n",
    "\n",
    "\n",
    "nmax = 50000  # maximum number of word embeddings to load\n",
    "\n",
    "# load monolingual word embeddings \n",
    "src_embeddings, src_id2word, src_word2id = load_embeddings(fr_path, nmax) # source = french \n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_embeddings(eng_path, nmax) # target = english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ground-truth bilingual dictionaries function\n",
    "def load_dic(path):\n",
    "    dico_full = {}\n",
    "    vectors_src=[]\n",
    "    vectors_tgt = []\n",
    "    with io.open(path,'r',encoding='utf_8') as f:\n",
    "        for i,line in enumerate(f):\n",
    "            word_src, word_tgt = line.rstrip().split(' ',1)\n",
    "            if word_tgt in tgt_word2id :\n",
    "                dico_full[word_src]=word_tgt\n",
    "    for key in dico_full.keys() :\n",
    "            vectors_src.append(src_embeddings[src_word2id[key]])\n",
    "            vectors_tgt.append(tgt_embeddings[tgt_word2id[dico_full[key]]])\n",
    "    X = np.vstack(vectors_src)\n",
    "    Z = np.vstack (vectors_tgt)\n",
    "    return dico_full,X,Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# train & test bilingual dictionaries\n",
    "path_train = '/Users/vince/DataProjetNLP/fr-en.0-5000.txt' \n",
    "path_test = '/Users/vince/DataProjetNLP/fr-en.5000-6500.txt'\n",
    "dico_train, X_train, Z_train = load_dic(path_train)\n",
    "dico_test, X_test, Z_test = load_dic(path_test) \n",
    "print(type(X_train[0]))\n",
    "X_train, Z_train, X_test, Z_test = map(torch.tensor, (X_train, Z_train, X_test, Z_test)) #TRANSFORM INTO TORCH TENSORS\n",
    "print(type(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to minimize \n",
    "# normalisation added\n",
    "def C(W,X,Z):\n",
    "    S = 0\n",
    "    S = sum(np.linalg.norm(np.dot(X, W.T) - Z, axis=1)**2) \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4971 training samples\n",
      "1483 test samples\n",
      "Vectors Dimension : 300\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[0], \"training samples\")\n",
    "print(X_test.shape[0], \"test samples\")\n",
    "dim = X_train.shape[1]\n",
    "print(\"Vectors Dimension :\", dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4971, 300])\n",
      "torch.Size([300, 4971])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "y = X_train.view(X_train.shape[1],-1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,d):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.h1 = nn.Linear(dim,2048,bias=True)\n",
    "        self.h2 = nn.Linear(2048,2048,bias=True)\n",
    "        self.out = nn.Linear(2048,1,bias=True)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = x.view(x.shape[0],-1) #Reshape \n",
    "        y = F.relu(self.h1(y))\n",
    "        y = F.relu(self.h2(y))\n",
    "        y = self.out(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mapping(nn.Module):\n",
    "    def __init__(self,d):\n",
    "        super(Mapping,self).__init__()\n",
    "        self.h1 = nn.Linear(dim,2048,bias=True)\n",
    "        self.h2 = nn.Linear(2048,2048,bias=True)\n",
    "        self.out = nn.Linear(2048,1,bias=True)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = x.view(x.shape[0],-1) #Reshape \n",
    "        y = F.relu(self.h1(y))\n",
    "        y = F.relu(self.h2(y))\n",
    "        y = self.out(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LossDisc():\n",
    "    return(-1/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LossMap():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrim = Discriminator(300)\n",
    "gen = Mapping(300)\n",
    "optimizer = torch.optim.SGD(discrim.parameters(),lr = 0.1)\n",
    "#criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (h1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "  (h2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  (out): Linear(in_features=2048, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "for n in range(n_epochs):\n",
    "    i = 0\n",
    "    for x, y in train_loader:\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        i+=1\n",
    "        if i % 100 ==0 :\n",
    "            print(\"Epoch [{}/{}], [{}/{}], batch loss = {}\".format(n+1,n_epochs,i*bs,n_data_train,loss))      \n",
    "            \n",
    "    acc = 0    \n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            y_pred = model(x)\n",
    "            y_pred = torch.argmax(y_pred,dim=1)\n",
    "            acc += (y_pred == y).sum().item()\n",
    "        acc = acc/n_data_test\n",
    "    print(\"Epoch [{}/{}], test accuracy = {}\".format(n+1,n_epochs,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
