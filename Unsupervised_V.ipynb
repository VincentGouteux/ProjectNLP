{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an efficient unsupervised word translator\n",
    "\n",
    "Based on : \"Word Translation Without Parallel Data\" by Alexis Conneau, Guillaume Lample, Marc Aurelio Ranzato, Ludovic Denoyer & Hervé Jégou (2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy.stats import special_ortho_group\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load function for pretrained versions of word embeddings\n",
    "def load_embeddings(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "eng_path = '/Users/louismonier/Downloads/Monolingual/wiki.en.vec' \n",
    "fr_path = '/Users/louismonier/Downloads/Monolingual/wiki.fr.vec'\n",
    "##########\n",
    "#eng_path = '/Users/vince/DataProjetNLP/wiki.en.vec'\n",
    "#fr_path = '/Users/vince/DataProjetNLP/wiki.fr.vec'\n",
    "##########\n",
    "nmax = 50000  # maximum number of word embeddings to load\n",
    "\n",
    "# load monolingual word embeddings \n",
    "src_embeddings, src_id2word, src_word2id = load_embeddings(fr_path, nmax) # source = french \n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_embeddings(eng_path, nmax) # target = english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ground-truth bilingual dictionaries function\n",
    "def load_dic(path):\n",
    "    dico_full = {}\n",
    "    vectors_src=[]\n",
    "    vectors_tgt = []\n",
    "    with io.open(path,'r',encoding='utf_8') as f:\n",
    "        for i,line in enumerate(f):\n",
    "            word_src, word_tgt = line.rstrip().split(' ',1)\n",
    "            if word_tgt in tgt_word2id :\n",
    "                dico_full[word_src]=word_tgt\n",
    "    for key in dico_full.keys() :\n",
    "            vectors_src.append(src_embeddings[src_word2id[key]])\n",
    "            vectors_tgt.append(tgt_embeddings[tgt_word2id[dico_full[key]]])\n",
    "    X = np.vstack(vectors_src)\n",
    "    Z = np.vstack (vectors_tgt)\n",
    "    return dico_full,X,Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# train & test bilingual dictionaries\n",
    "##########\n",
    "path_train = r'/Users/louismonier/Downloads/Monolingual/fr-en.0-5000.txt' \n",
    "path_test = r'/Users/louismonier/Downloads/Monolingual/fr-en.5000-6500.txt'\n",
    "##########\n",
    "#path_train = '/Users/vince/DataProjetNLP/fr-en.0-5000.txt' \n",
    "#path_test = '/Users/vince/DataProjetNLP/fr-en.5000-6500.txt'\n",
    "##########\n",
    "\n",
    "dico_train, X_train, Z_train = load_dic(path_train)\n",
    "dico_test, X_test, Z_test = load_dic(path_test)\n",
    "\n",
    "# convert embeddings vectors into torch tensors \n",
    "print(type(X_train[0]))\n",
    "X_train, Z_train, X_test, Z_test = map(torch.tensor, (X_train, Z_train, X_test, Z_test)) \n",
    "print(type(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4971 training samples\n",
      "1483 test samples\n",
      "Vectors dimension : 300\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[0], \"training samples\")\n",
    "print(X_test.shape[0], \"test samples\")\n",
    "dim = X_train.shape[1]\n",
    "print(\"Vectors dimension :\", dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the discriminator \n",
    "\n",
    "Recall what is the objective of the discriminator here : ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.h1 = nn.Linear(dim, 2048,bias=True) # 1st hidden layer\n",
    "        #self.h2 = nn.Linear(2048,2048,bias=True) # 2nd hidden layer\n",
    "        self.out = nn.Linear(2048,1,bias=True) # output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.dropout(x, p = 0.1) # dropout pour ajouter du bruit\n",
    "        x = F.leaky_relu(self.h1(x)) #, negative_slope=0.2)\n",
    "        #x = F.leaky_relu(self.h2(x), negative_slope=0.2)\n",
    "        y = torch.sigmoid(self.out(x)) # ouput = proba\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the generator \n",
    "\n",
    "Recall what is the objective of the discriminator here : ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple linear function \n",
    "# can be seen at a neural network whose weights are elements of W \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.l1 = nn.Linear(dim, dim, bias=False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = self.l1(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's time to train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrim = Discriminator(dim)\n",
    "gener = Generator(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizerD = optim.SGD(discrim.parameters(), lr=0.1)\n",
    "optimizerG = optim.SGD(gener.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for epoch in range(3): #3 Epochs \n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "N = 16 # nb of \"fake\" tgt // Wx\n",
    "M = 16 # nb of \"true\" tgt // y\n",
    "\n",
    "W = special_ortho_group.rvs(dim) #np.zeros((300,300), dtype=float)  #np.random.rand(Z_train.shape[1],Z_train.shape[1]) #  # #\n",
    "gener.l1.weight.data = torch.tensor(W,dtype=torch.float) # ini of generator's weights in SO(300)\n",
    "beta = 0.01\n",
    "\n",
    "for param in discrim.h1.parameters():\n",
    "    param.data.normal_(0.0, 0.2) #uniform_(-0.1,0.1)\n",
    "\n",
    "niter = 1000\n",
    "num_epochs = 5\n",
    "#nb_d = 1\n",
    "#nb_g = 10\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 0.8\n",
    "fake_label = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "[0/5][0/1000]\tLoss_D: 1.4459\tLoss_G: 2.1406\tD(x): 0.4791\tD(G(z)): 0.5109 / 0.0819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  app.launch_new_instance()\n",
      "/opt/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/5][50/1000]\tLoss_D: 16.3349\tLoss_G: 7.8043\tD(x): 0.9998\tD(G(z)): 1.0000 / 0.0040\n",
      "[0/5][100/1000]\tLoss_D: 8.1192\tLoss_G: 6.6719\tD(x): 0.9931\tD(G(z)): 0.9996 / 0.0006\n",
      "[0/5][150/1000]\tLoss_D: 5.3676\tLoss_G: 5.4549\tD(x): 0.9361\tD(G(z)): 0.9917 / 0.0021\n",
      "[0/5][200/1000]\tLoss_D: 3.4546\tLoss_G: 3.1207\tD(x): 0.8264\tD(G(z)): 0.9551 / 0.0299\n",
      "[0/5][250/1000]\tLoss_D: 3.3459\tLoss_G: 0.7574\tD(x): 0.0527\tD(G(z)): 0.1685 / 0.9444\n",
      "[0/5][300/1000]\tLoss_D: 3.3985\tLoss_G: 0.7100\tD(x): 0.0599\tD(G(z)): 0.2277 / 0.9172\n",
      "[0/5][350/1000]\tLoss_D: 2.2197\tLoss_G: 0.5495\tD(x): 0.2185\tD(G(z)): 0.3511 / 0.8380\n",
      "[0/5][400/1000]\tLoss_D: 2.2148\tLoss_G: 0.5602\tD(x): 0.2355\tD(G(z)): 0.4728 / 0.6748\n",
      "[0/5][450/1000]\tLoss_D: 1.9373\tLoss_G: 0.5358\tD(x): 0.2455\tD(G(z)): 0.3090 / 0.7295\n",
      "[0/5][500/1000]\tLoss_D: 1.6291\tLoss_G: 1.9228\tD(x): 0.7072\tD(G(z)): 0.6858 / 0.1394\n",
      "[0/5][550/1000]\tLoss_D: 1.8072\tLoss_G: 2.4322\tD(x): 0.5981\tD(G(z)): 0.7374 / 0.0683\n",
      "[0/5][600/1000]\tLoss_D: 1.9643\tLoss_G: 2.3997\tD(x): 0.6289\tD(G(z)): 0.7793 / 0.0762\n",
      "[0/5][650/1000]\tLoss_D: 1.9018\tLoss_G: 0.5705\tD(x): 0.2426\tD(G(z)): 0.3987 / 0.6478\n",
      "[0/5][700/1000]\tLoss_D: 1.6118\tLoss_G: 1.0049\tD(x): 0.4042\tD(G(z)): 0.5167 / 0.3334\n",
      "[0/5][750/1000]\tLoss_D: 1.4889\tLoss_G: 1.0688\tD(x): 0.4890\tD(G(z)): 0.4904 / 0.3136\n",
      "[0/5][800/1000]\tLoss_D: 1.5880\tLoss_G: 0.6813\tD(x): 0.3692\tD(G(z)): 0.4284 / 0.5202\n",
      "[0/5][850/1000]\tLoss_D: 1.6922\tLoss_G: 1.3186\tD(x): 0.5510\tD(G(z)): 0.6396 / 0.2230\n",
      "[0/5][900/1000]\tLoss_D: 1.5733\tLoss_G: 0.9415\tD(x): 0.4083\tD(G(z)): 0.4917 / 0.3675\n",
      "[0/5][950/1000]\tLoss_D: 1.4657\tLoss_G: 0.8924\tD(x): 0.4978\tD(G(z)): 0.4929 / 0.4025\n",
      "[1/5][0/1000]\tLoss_D: 1.4835\tLoss_G: 0.6339\tD(x): 0.3779\tD(G(z)): 0.3418 / 0.5625\n",
      "[1/5][50/1000]\tLoss_D: 1.6922\tLoss_G: 0.7177\tD(x): 0.4070\tD(G(z)): 0.5484 / 0.5069\n",
      "[1/5][100/1000]\tLoss_D: 1.4382\tLoss_G: 1.0692\tD(x): 0.5830\tD(G(z)): 0.5498 / 0.3108\n",
      "[1/5][150/1000]\tLoss_D: 1.6202\tLoss_G: 0.6953\tD(x): 0.3818\tD(G(z)): 0.5121 / 0.5150\n",
      "[1/5][200/1000]\tLoss_D: 1.5726\tLoss_G: 0.6900\tD(x): 0.3432\tD(G(z)): 0.3750 / 0.5158\n",
      "[1/5][250/1000]\tLoss_D: 1.5333\tLoss_G: 0.8679\tD(x): 0.4654\tD(G(z)): 0.5453 / 0.4116\n",
      "[1/5][300/1000]\tLoss_D: 1.5952\tLoss_G: 0.6335\tD(x): 0.3505\tD(G(z)): 0.4178 / 0.5697\n",
      "[1/5][350/1000]\tLoss_D: 1.5619\tLoss_G: 0.6669\tD(x): 0.3581\tD(G(z)): 0.3773 / 0.5358\n",
      "[1/5][400/1000]\tLoss_D: 1.6972\tLoss_G: 0.7908\tD(x): 0.4113\tD(G(z)): 0.5722 / 0.4434\n",
      "[1/5][450/1000]\tLoss_D: 1.5091\tLoss_G: 1.0440\tD(x): 0.5448\tD(G(z)): 0.5938 / 0.3150\n",
      "[1/5][500/1000]\tLoss_D: 1.6376\tLoss_G: 0.7358\tD(x): 0.3979\tD(G(z)): 0.5300 / 0.4786\n",
      "[1/5][550/1000]\tLoss_D: 1.4579\tLoss_G: 1.1718\tD(x): 0.5618\tD(G(z)): 0.5749 / 0.2750\n",
      "[1/5][600/1000]\tLoss_D: 1.4503\tLoss_G: 0.8654\tD(x): 0.5140\tD(G(z)): 0.5349 / 0.3967\n",
      "[1/5][650/1000]\tLoss_D: 1.5142\tLoss_G: 0.9599\tD(x): 0.4888\tD(G(z)): 0.5456 / 0.3544\n",
      "[1/5][700/1000]\tLoss_D: 1.4984\tLoss_G: 0.8712\tD(x): 0.4231\tD(G(z)): 0.4741 / 0.3999\n",
      "[1/5][750/1000]\tLoss_D: 1.5295\tLoss_G: 0.9421\tD(x): 0.4738\tD(G(z)): 0.5508 / 0.3650\n",
      "[1/5][800/1000]\tLoss_D: 1.4229\tLoss_G: 0.7785\tD(x): 0.5603\tD(G(z)): 0.5316 / 0.4471\n",
      "[1/5][850/1000]\tLoss_D: 1.4898\tLoss_G: 1.1833\tD(x): 0.5313\tD(G(z)): 0.5743 / 0.2866\n",
      "[1/5][900/1000]\tLoss_D: 1.5051\tLoss_G: 0.8520\tD(x): 0.4799\tD(G(z)): 0.5355 / 0.4085\n",
      "[1/5][950/1000]\tLoss_D: 1.5505\tLoss_G: 0.9623\tD(x): 0.5808\tD(G(z)): 0.6016 / 0.3550\n",
      "[2/5][0/1000]\tLoss_D: 1.5044\tLoss_G: 0.8573\tD(x): 0.4339\tD(G(z)): 0.4723 / 0.4026\n",
      "[2/5][50/1000]\tLoss_D: 1.4170\tLoss_G: 1.0479\tD(x): 0.5716\tD(G(z)): 0.5563 / 0.3073\n",
      "[2/5][100/1000]\tLoss_D: 1.5281\tLoss_G: 0.8449\tD(x): 0.4516\tD(G(z)): 0.5377 / 0.4055\n",
      "[2/5][150/1000]\tLoss_D: 1.5537\tLoss_G: 0.8216\tD(x): 0.4208\tD(G(z)): 0.4698 / 0.4253\n",
      "[2/5][200/1000]\tLoss_D: 1.5494\tLoss_G: 1.1656\tD(x): 0.5074\tD(G(z)): 0.5786 / 0.2967\n",
      "[2/5][250/1000]\tLoss_D: 1.5473\tLoss_G: 0.8096\tD(x): 0.4458\tD(G(z)): 0.5338 / 0.4333\n",
      "[2/5][300/1000]\tLoss_D: 1.5257\tLoss_G: 0.7438\tD(x): 0.5023\tD(G(z)): 0.5783 / 0.4715\n",
      "[2/5][350/1000]\tLoss_D: 1.4497\tLoss_G: 0.8013\tD(x): 0.4805\tD(G(z)): 0.5081 / 0.4381\n",
      "[2/5][400/1000]\tLoss_D: 1.6609\tLoss_G: 0.7407\tD(x): 0.3564\tD(G(z)): 0.4910 / 0.4727\n",
      "[2/5][450/1000]\tLoss_D: 1.5015\tLoss_G: 0.7855\tD(x): 0.5103\tD(G(z)): 0.5688 / 0.4401\n",
      "[2/5][500/1000]\tLoss_D: 1.5416\tLoss_G: 1.0236\tD(x): 0.5040\tD(G(z)): 0.5893 / 0.3351\n",
      "[2/5][550/1000]\tLoss_D: 1.4806\tLoss_G: 0.7587\tD(x): 0.4429\tD(G(z)): 0.4919 / 0.4606\n",
      "[2/5][600/1000]\tLoss_D: 1.4810\tLoss_G: 0.8180\tD(x): 0.4500\tD(G(z)): 0.4897 / 0.4289\n",
      "[2/5][650/1000]\tLoss_D: 1.4808\tLoss_G: 1.0045\tD(x): 0.5119\tD(G(z)): 0.5546 / 0.3252\n",
      "[2/5][700/1000]\tLoss_D: 1.5368\tLoss_G: 1.5677\tD(x): 0.6088\tD(G(z)): 0.6386 / 0.1817\n",
      "[2/5][750/1000]\tLoss_D: 1.5162\tLoss_G: 0.7305\tD(x): 0.4377\tD(G(z)): 0.5047 / 0.4743\n",
      "[2/5][800/1000]\tLoss_D: 1.6082\tLoss_G: 0.8634\tD(x): 0.4337\tD(G(z)): 0.5724 / 0.3939\n",
      "[2/5][850/1000]\tLoss_D: 1.4571\tLoss_G: 0.8044\tD(x): 0.4337\tD(G(z)): 0.4314 / 0.4452\n",
      "[2/5][900/1000]\tLoss_D: 1.5207\tLoss_G: 0.7779\tD(x): 0.4331\tD(G(z)): 0.4996 / 0.4484\n",
      "[2/5][950/1000]\tLoss_D: 1.4855\tLoss_G: 0.7418\tD(x): 0.4274\tD(G(z)): 0.4396 / 0.4718\n",
      "[3/5][0/1000]\tLoss_D: 1.5251\tLoss_G: 0.7314\tD(x): 0.4270\tD(G(z)): 0.5032 / 0.4866\n",
      "[3/5][50/1000]\tLoss_D: 1.5322\tLoss_G: 0.8136\tD(x): 0.5458\tD(G(z)): 0.5969 / 0.4351\n",
      "[3/5][100/1000]\tLoss_D: 1.5018\tLoss_G: 0.9050\tD(x): 0.4867\tD(G(z)): 0.5510 / 0.3811\n",
      "[3/5][150/1000]\tLoss_D: 1.4932\tLoss_G: 0.8672\tD(x): 0.5771\tD(G(z)): 0.5897 / 0.3974\n",
      "[3/5][200/1000]\tLoss_D: 1.5362\tLoss_G: 0.7159\tD(x): 0.4576\tD(G(z)): 0.5304 / 0.4941\n",
      "[3/5][250/1000]\tLoss_D: 1.4721\tLoss_G: 0.7557\tD(x): 0.4853\tD(G(z)): 0.5241 / 0.4623\n",
      "[3/5][300/1000]\tLoss_D: 1.5044\tLoss_G: 0.8516\tD(x): 0.4540\tD(G(z)): 0.5044 / 0.4129\n",
      "[3/5][350/1000]\tLoss_D: 1.4235\tLoss_G: 0.7528\tD(x): 0.4639\tD(G(z)): 0.4656 / 0.4602\n",
      "[3/5][400/1000]\tLoss_D: 1.4550\tLoss_G: 0.8163\tD(x): 0.5012\tD(G(z)): 0.5209 / 0.4202\n",
      "[3/5][450/1000]\tLoss_D: 1.5114\tLoss_G: 0.7879\tD(x): 0.4737\tD(G(z)): 0.5346 / 0.4391\n",
      "[3/5][500/1000]\tLoss_D: 1.6338\tLoss_G: 0.7045\tD(x): 0.3893\tD(G(z)): 0.4936 / 0.5011\n",
      "[3/5][550/1000]\tLoss_D: 1.5163\tLoss_G: 0.8862\tD(x): 0.5164\tD(G(z)): 0.5856 / 0.3819\n",
      "[3/5][600/1000]\tLoss_D: 1.4696\tLoss_G: 0.8216\tD(x): 0.5161\tD(G(z)): 0.5474 / 0.4158\n",
      "[3/5][650/1000]\tLoss_D: 1.4886\tLoss_G: 0.8072\tD(x): 0.5324\tD(G(z)): 0.5803 / 0.4282\n",
      "[3/5][700/1000]\tLoss_D: 1.5429\tLoss_G: 0.7724\tD(x): 0.4611\tD(G(z)): 0.5426 / 0.4557\n",
      "[3/5][750/1000]\tLoss_D: 1.4904\tLoss_G: 0.7869\tD(x): 0.4727\tD(G(z)): 0.5238 / 0.4427\n",
      "[3/5][800/1000]\tLoss_D: 1.4576\tLoss_G: 0.9181\tD(x): 0.4877\tD(G(z)): 0.5220 / 0.3710\n",
      "[3/5][850/1000]\tLoss_D: 1.4908\tLoss_G: 0.7699\tD(x): 0.4318\tD(G(z)): 0.4801 / 0.4498\n",
      "[3/5][900/1000]\tLoss_D: 1.5358\tLoss_G: 0.7606\tD(x): 0.5203\tD(G(z)): 0.6032 / 0.4546\n",
      "[3/5][950/1000]\tLoss_D: 1.5225\tLoss_G: 0.7467\tD(x): 0.4134\tD(G(z)): 0.4767 / 0.4685\n",
      "[4/5][0/1000]\tLoss_D: 1.4399\tLoss_G: 0.8898\tD(x): 0.5324\tD(G(z)): 0.5494 / 0.3820\n",
      "[4/5][50/1000]\tLoss_D: 1.5097\tLoss_G: 0.8422\tD(x): 0.4601\tD(G(z)): 0.5033 / 0.4228\n",
      "[4/5][100/1000]\tLoss_D: 1.4207\tLoss_G: 0.7406\tD(x): 0.4717\tD(G(z)): 0.4801 / 0.4737\n",
      "[4/5][150/1000]\tLoss_D: 1.4877\tLoss_G: 0.7554\tD(x): 0.4364\tD(G(z)): 0.4913 / 0.4605\n",
      "[4/5][200/1000]\tLoss_D: 1.5091\tLoss_G: 0.7402\tD(x): 0.4108\tD(G(z)): 0.4589 / 0.4766\n",
      "[4/5][250/1000]\tLoss_D: 1.4647\tLoss_G: 0.8452\tD(x): 0.4700\tD(G(z)): 0.4716 / 0.4075\n",
      "[4/5][300/1000]\tLoss_D: 1.4757\tLoss_G: 0.7985\tD(x): 0.5153\tD(G(z)): 0.5500 / 0.4421\n",
      "[4/5][350/1000]\tLoss_D: 1.4813\tLoss_G: 0.8117\tD(x): 0.4599\tD(G(z)): 0.4841 / 0.4271\n",
      "[4/5][400/1000]\tLoss_D: 1.4458\tLoss_G: 0.8200\tD(x): 0.5313\tD(G(z)): 0.5507 / 0.4175\n",
      "[4/5][450/1000]\tLoss_D: 1.4587\tLoss_G: 0.9320\tD(x): 0.5165\tD(G(z)): 0.5434 / 0.3634\n",
      "[4/5][500/1000]\tLoss_D: 1.4171\tLoss_G: 0.7729\tD(x): 0.4979\tD(G(z)): 0.4869 / 0.4499\n",
      "[4/5][550/1000]\tLoss_D: 1.4899\tLoss_G: 0.7832\tD(x): 0.4901\tD(G(z)): 0.5516 / 0.4463\n",
      "[4/5][600/1000]\tLoss_D: 1.4667\tLoss_G: 0.7651\tD(x): 0.5043\tD(G(z)): 0.5394 / 0.4573\n",
      "[4/5][650/1000]\tLoss_D: 1.5207\tLoss_G: 0.7737\tD(x): 0.4608\tD(G(z)): 0.5235 / 0.4549\n",
      "[4/5][700/1000]\tLoss_D: 1.4205\tLoss_G: 0.7387\tD(x): 0.4959\tD(G(z)): 0.4850 / 0.4853\n",
      "[4/5][750/1000]\tLoss_D: 1.5215\tLoss_G: 0.6959\tD(x): 0.4154\tD(G(z)): 0.4663 / 0.5140\n",
      "[4/5][800/1000]\tLoss_D: 1.5064\tLoss_G: 0.8101\tD(x): 0.4714\tD(G(z)): 0.5322 / 0.4315\n",
      "[4/5][850/1000]\tLoss_D: 1.4860\tLoss_G: 0.8158\tD(x): 0.5249\tD(G(z)): 0.5716 / 0.4286\n",
      "[4/5][900/1000]\tLoss_D: 1.5156\tLoss_G: 0.7511\tD(x): 0.4279\tD(G(z)): 0.4681 / 0.4683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/5][950/1000]\tLoss_D: 1.4325\tLoss_G: 0.7975\tD(x): 0.5545\tD(G(z)): 0.5624 / 0.4368\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(niter):\n",
    "        \n",
    "        ###############################################################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z))) #\n",
    "        ###############################################################\n",
    "        \n",
    "        ## Train with all-real batch\n",
    "        discrim.zero_grad()\n",
    "        # Format batch : generate M random words from the target\n",
    "        \n",
    "        rand_tgt_word_id = torch.Tensor(M).random_(X_train.shape[0]).long()\n",
    "        tgt_word_emb = Z_train[rand_tgt_word_id.numpy()]\n",
    "        tgt_word_emb = torch.tensor(tgt_word_emb, dtype=torch.float) # conversion to tensor\n",
    "\n",
    "        label = torch.full((M,1), real_label)\n",
    "        \n",
    "        # Forward pass real batch through D\n",
    "        output = discrim(tgt_word_emb)#.view(-1)\n",
    "        #print(output.shape)\n",
    "        #print(output.view(-1).shape)\n",
    "        \n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        \n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "        \n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        \n",
    "        ## create a new batch of N fake & M true data to train the discriminator\n",
    "        # generate N random words from the source \n",
    "        rand_src_word_id = torch.Tensor(N).random_(X_train.shape[0]).long()\n",
    "        src_word_emb = X_train[rand_src_word_id.numpy()]\n",
    "        noise = torch.tensor(src_word_emb, dtype=torch.float) # conversion to tensor\n",
    "        \n",
    "        # Generate fake tgt batch with G\n",
    "        fake = gener(noise)\n",
    "        \n",
    "        label.fill_(fake_label)  \n",
    "        # Classify all fake batch with D\n",
    "        output = discrim(fake.detach())#.view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Add the gradients from the all-real and all-fake batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "        \n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        gener.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = discrim(fake)#.view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "        \n",
    "        W_trained = gener.l1.weight.data # get the weights of the generator which are the elements of W\n",
    "        # to ensure that the matrix stays close to the manifold of orthogonal matrices after each update\n",
    "        W_ortho = (1+beta)*W_trained - beta*torch.mm(torch.mm(W_trained, W_trained.t()), W_trained) \n",
    "        gener.l1.weight.data = W_ortho \n",
    "        \n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, niter,\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        \n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAFNCAYAAADRi2EuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdd3xV9f3H8dcnmxFWCHsEWQoqKIjinmi1Vm21Kg60VutA22q1jrqrVetoHT+texW3uMBBUVAUZcveM6wkQMhe935/f5yTkEB2cu9N4P18PPLIvWd8z+ecc8fnfr/f8z3mnENEREREQi8q0gGIiIiI7CuUeImIiIiEiRIvERERkTBR4iUiIiISJkq8RERERMJEiZeIiIhImCjxEtkHmdk9ZvZmA8vIMbP9Gismv8zPzWxMPdd9zszubMx4pGpmtsjMjo90HDUxszvN7LnGXlakvkzjeElzYWYXAH8GDgRygTXAa8Czrom9kM1sCvCmc+7FSMdSGTO7B+jnnLu4knnHA18Def6kTOAH4J/OuZnhijFSzCwF77UV65wraaQyj8d7PfRojPLquO0pwBFAMeCAFcB7wBPOucJwx1MdM7sduN1/GgPEAvn+83XOucERCUykEanGS5oFM7sJ+DfwT6AL0Bm4GjgKiAtzLDEhLt/MLNLvzU3OudZAIt6X9lLgOzM7KRQbayL73ChC/fqop7HOuUSgK3ATcAEw0cysrgWFcv+ccw8651r7r72rgemlzytLuprosRap1l7xQSd7NzNrC9wHXOuce985l+08c51zF5X+ajezeDN71MzWm9lWv+mphT/veDNLNbObzCzNzDab2eXltlGbdf9qZluAV8ysvZl9ZmbpZrbDf9zDX/4B4Bjgab857ml/+pFmNtPMdvr/jyy3/Slm9oCZfY9X07RHE56Z3Wpmq8ws28wWm9k55eZdZmbT/H3YYWZrzOwX5eb3MbOp/rqTgI61Ofb+cU51zt0FvAg8XK5MZ2b9/Men+zFlm9lGM/tLueXOMrN5Zpblx39aVfvsT/t9uX363syeMLNMM1vtH8PLzGyDfx7HlNvOq2b291qe7zPMbK4f0wa/BrDUt/7/TP/8jTSzKDP7m5mt88t73X9dYmYp/rG4wszW49UW1pqZtfXLS/fL/1tpEmpm/fzzttPMMszsHX+6+cclzZ8338wOrGlbzrlc59wU4FfASOCM3Y9d+eNX7vla//U/H8g1sxh/2sn+/HvM7F1/P7LNa4YcXm79Q/3jnW1m75nZO+W3V4djFeMf62vNbCXeDwLM7Gn/fGdV8t76u5m9Wu54OjO71F8+3cxureeyLc3sTf+1udi89+fauu6T7HuUeElzMBKIBz6uYbmHgQHAUKAf0B24q9z8LkBbf/oVwDNm1r4O63YAegNX4b13XvGf98JrDnkawDl3B/AdXi1Da+fcWDPrAEwAngSSgMeBCWaWVG4bl/hlJwLrKtm/VXgJXVvgXuBNM+tabv7hwDK8pOoR4CWzshqNccBsf979QH36UX0IHGpmrSqZ9xLwB79W5UD85MPMRgCvAzcD7YBjgbXl1qtpnw8H5uMds3HA28BheOfoYrzktnUV8VZ3vnOBS/2YzgCuMbOz/XnH+v/b+edvOnCZ/3cCXlLcGv98l3MccABwahXxVOUpP879/DIuBUqTxPuBr4D2QA9/WYBRfpwD/H04H9hW2w0659YDs/BeT7V1Id6xaldFE+yv8M5PO+AT/ONjZnHAeOBVvPfQW8A5laxfF7/Cex0c5D//CTjYL/994D0zi69m/SPxXkOnAveaWf96LHsf0A1I8eft0WwvUhklXtIcdAQyyn/Ym9kP/i/NfDM71k8wrgT+7Jzb7pzLBh7Ea1IpVQzc55wrds5NBHKAgbVcNwjc7ZwrdM7lO+e2Oec+cM7l+cs/gPelWZUzgBXOuTeccyXOubfwfq2fWW6ZV51zi/z5xbsX4Jx7zzm3yTkXdM69g9dXZ0S5RdY5515wzgXw+r51BTqbWS+8L6k7/fi/BT6tJtaqbAIM74t1d8XAIDNr45zb4Zyb40+/AnjZOTfJj3ujc25pbfcZWOOce8Xfp3eAnnjnsNA59xVQhPelWJlKzzeAc26Kc26BH9N8vGSguvN3EfC4c261cy4HuA24wCo2dd3j1yjlV17EnswsGi9pus2vyV0LPIaXkJbuQ2+gm3OuwDk3rdz0RGB/vL66S5xzm2u7Xd8mvESltp50zm2oZv+mOecm+ufqDWCIP/0IvP5aT/rn4kNgRh1j3d2D/ussH8B/X233PyMeAdpQ9esCvHNV4L9OF5WLtS7L/hZ4wDmX6ZzbwJ6JuEillHhJc7AN6Fj+S845d6Rzrp0/LwpIBloCs/2ELBP4wp9eVs5uv9Tz8GouarNuunOuoPSJ38zwH79pKAuveaqd/0VamW7sWaOzDq82ptSG6g6C3+Qxr1yMB1KxyXBL6QPnXGnH+Nb+tnc453J323ZddcfrnJ1ZybzfAKcD6/ymsZH+9J54NXVVqXafga3lHpd+ye4+raoar6rON2Z2uJl94zcf7cTrT1Rd8+vu528dXjLRudy0mvalMh3x+ijuXnbp6+IWvGR3ht989zsA59zXeF/0zwBbzex5M2tTx213B7bXYfma9m9Lucd5QIL/nu0GbNztApj6HKsqYzGzW8xsqX8udwCtqOZ8Oud2j7Wq11B1y3bdLY6G7pPsI5R4SXMwHSgEzqpmmQy8L+HBzrl2/l9bv5NuTWqz7u5XTd6EV3tyuHOuDbuap6yK5Tfh1VyU1wvYWM02yphZb+AFYCyQ5CedC8ttrzqbgfa7NRH2qsV6uzsHmLNbAgeAc26mc+4soBPwEfCuP2sD0LeaMiN1Neo4vOawns65tsBzVH3uYM/z1wsooWJiWJ99yWBXrVb5sjeC96XvnLvSOdcN+APwf+b3q3POPemcGwYMxmtyvLm2GzWznsAwvCZx8JpeW5ZbpEslq9X3XG0Gupdr9gYvIW+IsljM7ATgRrzkvx1es2wOtXtvNMQWvObfUg3dJ9lHKPGSJs85l4nXp+n/zOxcM2ttXmfnoXi/bHHOBfESkyfMrBOAmXU3sxr729Rz3US8ZC3T7791927zt1Kxg/xEYICZjfY7CJ8PDAI+q/EAeFrhfdmk+/FdjlfjVSPn3Dq8/jz3mlmcmR1NxSbOKpmnu5ndDfyeXZf6l18mzswuMrO2fnNhFhDwZ78EXG5mJ/nnrLuZ7V+bbYdYIrDdOVfg90MbXW5eOl7Tcvnz9xbwZ/MuUmiN1xT9ThV9napkZgnl//ztvAs8YGaJfoJ9I/Cmv/x55l+0gVeT44CAmR3m19rF4iVNBew65tVtv6WZHYfXX3IG3usSYB5wupl1MLMuwJ/qsl81mO7HNtZ/7Z9FxSbyhkrES4Iz8IafuAf/cyHE3gVuN7N2/jm6LgzblL2AEi9pFpxzj+B9Id0CpOElNv8B/oo3xhT+45XAj37z3//w+/TUQl3X/RfQAu/D/ke8psny/g2ca94Vhk8657YBv8SrKdvm78cvnXMZtQnOObcYr+/PdLx9Pwj4vpb7Bl5icThe09LdeB3eq9PNzHLwag5m+ts73u9XVZlLgLX+sbsav6Oxc24GXkfxJ4CdwFT2rPmLhGuB+8wsG+8iitIautJm2geA7/1m3SOAl/H6LX2LN8ZXAXB9HbfZHS9ZL//X1y8nF1gNTMOrjXvZX+cw4Cf/XHwC/NE5twavD9MLeMnYOrzX1KPVbPtpf1+34r12PwBO83904O/bz3gXPnyF15+uUTjnioBf4/X3y8R7bXyGV4vdGCbivV9X4MWfhVfLFmp34x3PtXjH7F0ab59kL6YBVEVEJKzM7CfgOefcK5GOpbGY2fXA2c65kIx1J3sP1XiJiEhImdlxZtbFb2ocgzf0w+61xM2K32x+pN+EfgDeXTXGRzouafo06q+IiITaQLymuNZ4V7meW4/hL5qaeLzm3hS8Jt+38Lo/iFRLTY0iIiIiYaKmRhEREZEwUeIlIiIiEibNoo9Xx44dXUpKSqTDEBEREanR7NmzM5xzyZXNaxaJV0pKCrNmzYp0GCIiIiI1MrMqb8umpkYRERGRMFHiJSIiIhImSrxEREREwqRZ9PESERGRfUNxcTGpqakUFBREOpQaJSQk0KNHD2JjY2u9jhIvERERaTJSU1NJTEwkJSUFM4t0OFVyzrFt2zZSU1Pp06dPrddTU6OIiIg0GQUFBSQlJTXppAvAzEhKSqpzzZwSLxEREWlSmnrSVao+cSrxEhEREdnN1q1bGT16NPvttx/Dhg1j5MiRjB8/vsHlKvESERERKcc5x9lnn82xxx7L6tWrmT17Nm+//TapqakNLluJ1+4yVsCOKgecFRERkb3c119/TVxcHFdffXXZtN69e3P99dc3uGxd1bi7p4d7/+/ZGdk4REREJCIWLVrEoYceGpKylXiJiIhIk3Tvp4tYvCmrUcsc1K0Nd585uE7rXHfddUybNo24uDhmzpzZoO2rqbFUoATuaRvpKERERCTCBg8ezJw5c8qeP/PMM0yePJn09PQGl60ar1JTH4p0BCIiIlJOXWumGsuJJ57I7bffzrPPPss111wDQF5eXqOUrRqvUvPfjXQEIiIi0gSYGR999BFTp06lT58+jBgxgjFjxvDwww83uGzVeImIiIjspmvXrrz99tuNXq5qvEplaggJERERCS0lXgBpSyIdgYiIiOwDlHgBZG+OdAQiIiKyD1DiBUDzuBmniIiING9KvERERETCRIkXgKnGS0REREJPiZeIiIhIOdHR0QwdOpTBgwczZMgQHn/8cYLBYKOUrXG8gEr7eM18Cfb/JSR2Dn84IiIiEjEtWrRg3rx5AKSlpTF69Gh27tzJvffe2+CyVeNVlQk3wjsXRzoKERERiaBOnTrx/PPP8/TTT+Oca3B5Sryg6j5euQ2/GaaIiIg0b/vttx/BYJC0tLQGl6WmxursWBPpCERERPZdn98KWxY0bpldDoJfPFTn1RqjtgtCWONlZj3N7BszW2Jmi8zsj/70e8xso5nN8/9OD1UMjaIoN9IRiIiISAStXr2a6OhoOnXq1OCyQlnjVQLc5JybY2aJwGwzm+TPe8I592gIt11H1Qwn8fpZcPI9kHJ0uIIRERERqFfNVGNLT0/n6quvZuzYsVgjDD8VssTLObcZ2Ow/zjazJUD3UG2vQao7kKkz4b3L4OaVYQtHREREIic/P5+hQ4dSXFxMTEwMl1xyCTfeeGOjlB2WPl5mlgIcAvwEHAWMNbNLgVl4tWI7whGHiIiISE0CgUDIyg75VY1m1hr4APiTcy4LeBboCwzFqxF7rIr1rjKzWWY2Kz09wlcXNlKHOhEREdm3hTTxMrNYvKTrv865DwGcc1udcwHnXBB4ARhR2brOueedc8Odc8OTk5NDGSa6SbaIiIiEQyivajTgJWCJc+7xctO7llvsHGBhqGKotRo7y6nGS0RERBoulH28jgIuARaY2Tx/2u3AhWY2FC+bWQv8IYQxiIiISDPjnGuUKwhDrT5je4XyqsZpVN6GNzFU26y/pn9yRURE9gUJCQls27aNpKSkJp18OefYtm0bCQkJdVpPI9eLiIhIk9GjRw9SU1OJ+IV1tZCQkECPHj3qtI4SL6i5j5euahQREQmL2NhY+vTpE+kwQkY3yRYREREJEyVeQM19vFTjJSIiIg2nxEtEREQkTJR4QS3G8RIRERFpOCVeIiIiImGixAvQOF4iIiISDkq8akPDSYiIiEgjUOIlIiIiEiZKvECd60VERCQslHgB6uMlIiIi4aDEq1bUx0tEREQaTomXiIiISJgo8QLdMUhERETCQokXoD5eIiIiEg5KvERERETCRIkXaDgJERERCQslXiIiIiJhosSrVtS7XkRERBpOiZeIiIhImCjxgppvgq2bZIuIiEgjUOIlIiIiEiZKvEBXNYqIiEhYKPESERERCRMlXrWiPl4iIiLScEq8QJ3nRUREJCyUeNWGEjMRERFpBEq8QJ3rRUREJCyUeImIiIiEiRIvUFOiiIiIhIUSLxEREZEwUeJVK6oRExERkYZT4iUiIiISJkq8QFc1ioiISFgo8QJ1rhcREZGwUOJVG0rMREREpBGELPEys55m9o2ZLTGzRWb2R396BzObZGYr/P/tQxWDiIiISFMSyhqvEuAm59wBwBHAdWY2CLgVmOyc6w9M9p83carxEhERkYYLWeLlnNvsnJvjP84GlgDdgbOA1/zFXgPODlUMIiIiIk1JWPp4mVkKcAjwE9DZObcZvOQM6BSOGEREREQiLeSJl5m1Bj4A/uScy6rDeleZ2Swzm5Wenh66AEVERETCJKSJl5nF4iVd/3XOfehP3mpmXf35XYG0ytZ1zj3vnBvunBuenJwcyjBrpqsaRUREpBGE8qpGA14CljjnHi836xNgjP94DPBxqGIQERERaUpiQlj2UcAlwAIzm+dPux14CHjXzK4A1gPnhTAGERERkSYjZImXc24aUNW9eE4K1XZDQ02NIiIi0nAauV5EREQkTJR4iYiIiISJEi8RERGRMFHiVRsaTkJEREQagRIvERERkTBR4iUiIiISJkq8RERERMJEiZeIiIhImCjxqhV1rhcREZGGU+JVG8GSSEcgIiIiewElXiIiIiJhosQLUFOiiIiIhIMSLxEREZEwUeIlIiIiEiZKvACwSAcgIiIi+wAlXoD6eImIiEg4KPESERERCRMlXiIiIiJhosRLREREJEyUeImIiIiEiRIvUN96ERERCQslXiIiIiJhosRLREREJEyUeNVWfmakIxAREZFmTokXUKtOXg/3hqUTQh+KiIiI7LWUeNXF2mmRjkBERESaMSVeIiIiImGixEtEREQkTJR41YlFOgARERFpxpR4AbjajqCqkVZFRESk/pR4iYiIiISJEq86UVOjiIiI1J8SLxEREZEwUeIFqO+WiIiIhIMSLxEREZEwUeIlIiIiEiZKvERERETCJGSJl5m9bGZpZraw3LR7zGyjmc3z/04P1fZDwnRVo4iIiNRfKGu8XgVOq2T6E865of7fxBBuv/ZqO4BqrQdaFREREdlTyBIv59y3wPZQlS8iIiLS3ESij9dYM5vvN0W2j8D2609NjSIiItIA4U68ngX6AkOBzcBjVS1oZleZ2Swzm5Wenh6u+ERERERCJqyJl3Nuq3Mu4JwLAi8AI6pZ9nnn3HDn3PDk5ORQR1arpXbkFoY4DhEREdmb1SrxMrO+ZhbvPz7ezG4ws3Z13ZiZdS339BxgYVXLNkUfzEmNdAgiIiLSjNW2xusDIGBm/YCXgD7AuOpWMLO3gOnAQDNLNbMrgEfMbIGZzQdOAP5c/9DDz+km2SIiItIAMbVcLuicKzGzc4B/OeeeMrO51a3gnLuwkskv1TlCERERkb1EbWu8is3sQmAM8Jk/LTY0IYmIiIjsnWqbeF0OjAQecM6tMbM+wJuhCyvMNDCqiIiIhEGtmhqdc4uBGwD8sbcSnXMPhTIwERERkb1Nba9qnGJmbcysA/Az8IqZPR7a0ERERET2LrVtamzrnMsCfg284pwbBpwcurCaJl3VKCIiIg1R28Qrxh+D67fs6ly/F6ldHy+r5XIiIiIilalt4nUf8CWwyjk308z2A1aELiwRERGRvU9tO9e/B7xX7vlq4DehCqqpUlOjiIiINERtO9f3MLPxZpZmZlvN7AMz6xHq4ERERET2JrVtanwF+AToBnQHPvWniYiIiEgt1TbxSnbOveKcK/H/XgWSQxhXeGkAVREREQmD2iZeGWZ2sZlF+38XA9tCGZiIiIjI3qa2idfv8IaS2AJsBs7Fu42QiIiIiNRSrRIv59x659yvnHPJzrlOzrmz8QZT3aeoQVJEREQaorY1XpW5sdGiiLjapVQXRn8N21aFOBYRERHZWzUk8drnBrVqY/nw4kmRDkNERESaqYYkXvtmy1v+jkhHICIiIs1UtSPXm1k2lSdYBrQISUQiIiIie6lqEy/nXGK4AhERERHZ2zWkqXHvoQFURUREJAyUeImIiIiEiRIvERERkTBR4iUiIiISJkq8gH11ZAwREREJLyVeIiIiImGixEtEREQkTJR4iYiIiISJEi8RERGRMFHiBRpAVURERMJCiZeIiIhImCjxEhEREQkTJV5AUSAY6RBERERkH6DEC3hn5vpIhyAiIiL7ACVeQEZ2UZ2Wz9+6gpRbJ/D01ytCFJGIiIjsjZR4AWZ1Wz7mvUsB+O9PqikTERGR2lPiRd0TL5z6hImIiEjdKfESERERCRMlXgDUbQDV2G1LQxSHiIiI7M1ClniZ2ctmlmZmC8tN62Bmk8xshf+/fai2XxdGXdsaIYHCEEQiIiIie7NQ1ni9Cpy227RbgcnOuf7AZP95s2R1rCUTERERCVni5Zz7Fti+2+SzgNf8x68BZ4dq+6Hm6lFLJiIiIvu2cPfx6uyc2wzg/+8U5u1XKkq1VyIiIhIGTbZzvZldZWazzGxWenp6pMPZw0XRk+kbXBvpMERERKQZCXfitdXMugL4/9OqWtA597xzbrhzbnhycnJoo6rzQF5wZ+ybvFl8YwiCERERkb1VuBOvT4Ax/uMxwMdh3r6IiIhIxIRyOIm3gOnAQDNLNbMrgIeAU8xsBXCK/zzi1E1eREREwiEmVAU75y6sYtZJodpmfWloCBEREQmHJtu5PqxU5SUiIiJhoMQL5V0iIiISHkq8GigYdASCaqoUERGRminxAszVP3G6+KWf6Hv7xEaMRkRERPZWSrygXuN4lfph1bZGDERERET2Zkq8RERERMJEiRfqXC8iIiLhocQLMFPneBEREQk9JV4NdE/Mq6xNGB3pMERERKQZUOIFNKSx8bKYrxoxDhEREdmbKfESERERCRMlXiIiIiJhosQLiNJNskVERCQMlHiJiIiIhIkSLxEREZEwUeJFg+4YJCIiIlJrSrxopJHrG3CjbREREdk3KPFqLEq8REREpAZKvBqNEi8RERGpnhIvdJNsERERCQ8lXjRS53q/qXF7bhFOzY4iIiJSCSVeQOM0EzqWbM7i0Psn8d6s1ApzJi/ZSsqtE9iZX9wI2xEREZHmSokX0CiNjc6xfGs2AN+tzKgw6+lvVgKwMi2n4dsRERGRZkuJF9BYNV7h2Y6IiIg0V0q8GsvG2azJyK0wKRh0LNy4M0IBiYiISFMTE+kAmoJGuarxlV/wr4JxFSa9NG0ND0xc0thbEhERkWZKNV4Qsnxo8eas0BQsIiIizZISrzp6tPi8auaqD5eIiIhUTU2N1K3Ca53rXOW806Jm8kVwBAApt05oYFQiIiKyt1GNVx1Vl6R1sGz6W6ru2ygiIiKVUuIFWCMlSkNsFZPib+HEHe80SnkiIiKyd1HiVUc/Bg+ocl438wZO3S9/UbjCERERkWZEiRfUqZNXGu2rnFdCNADRrqpbA6kJUkREZF+mxAuwRhpPYlfiVdIo5YmIiMjeRYkXYI1UE+X8BC6KYKOUJyIiInsXJV7QaAOoxqKaLhEREamaEi+oc9er64vGVjr9hOifASgoyK9iTd0ySEREZF8WkQFUzWwtkA0EgBLn3PBIxFFfGbStdr4rKQxTJCIiItKcRHLk+hOccxkR3P4udayImh4cVO38KF29KCIiIpVQUyNgfp50WdEtjCn6a23WqGFuxcSrHdk8EvMfoorz6hmhiIiI7A0ilXg54Cszm21mV0Uohj2ku3ZscVWP01VbB0WtrfD8zzHv89uYqSSteK/BZYuIiEjzFammxqOcc5vMrBMwycyWOue+Lb+An5BdBdCrV6+QBmMh7vMeXTq8RKg3JCIiIk1aRGq8nHOb/P9pwHhgRCXLPO+cG+6cG56cnBzuECu1LtipTsvHUsKNMe/S2ryrHJ1FhyIsERERaSbCXuNlZq2AKOdctv94FHBfuOOoEFO5PlnLXY8ql8ulRZ3KPT/6G26I+ajsuYtSlzoREZF9WSSaGjsD481rdosBxjnnvohAHHtwgKumEjBYh8sfD7LVJFC021QlXiIiIvuysGcCzrnVzrkh/t9g59wD4Y6hvgLlDtf+Ba/wTWAIr5ecUumyn8b/jQOi1lWYpqZGERGRfVskx/FqdoLlEq8C4rm82Bt64pPASN6P37O19DfR0ypOUFOjiIjIPk2ZANR6ANVAFYdrthtQq/UdqvESERHZlynxYtcAqjX5IHBMpdOr6xdWcUMVl7t9/AKufH1W2fO563eQcusEZqzZXrvyREREpFlR4lWOq6Hq663AiQ0rP6pijde4n9YzafFWLntlBjvzi/l+pXcHpanL0xq0HREREWmalHjVScMGQLVgCTgHc96Awpyy6VOWpTN+TmpDgxMREZEmTp3rw2i/qTdA797wyVhY9wPwy0iHJCIiImGkGi8gKpx38slJ9/9vqTB5dUYuzu9rtmJrDiIiIrL3UeJVTk19vBpDMFDsbyxYYfrr09eRme/N+2rx1pDHISIiIuGnxCvMvl68EYBpK9L3mLcgdWe4wxEREZEwUuJFQ7vM103+utkARPn3h1ybMJpro737Oc5YW/thJHIKS0i5dQLvztrQ+EGKiIhISCjxCrMziz4HvGQvCq+58ZbYdzk1amadytmyMx+A/0xd1ajxiYiISOgo8QKg+hFUpwYO5pWSU6tdZnpgUJ22ODJ6MasTLi57/mTsU3RlW53KEBERkeZFw0mUU5p+XVD0N9qSw3/i/gXAmOJba1z3kuJbWRl9ab23HW8lTE+4HoC/F18EnFHpcgXFATJyCuu9HREREYkc1XhV4sfgIL4MjuDm4qt4tuTMWq1TUi6HPaPwwQZt/8aY9+l7+8RK5135+iyOfvibatcvKA7w1OQVFAeC1S4nIiIi4aXEC7Aqete/Fzieh0surHN5i1xKg+JpaYUEgo7MvCIA1m3L5atF3rhf363IqHH9p79eyWOTlvP2zIod751zPDd1FVuzChoUn4iIiNSPEi9Ce1XjmYV/r9d6axNGs2bpPABOeHQKV73hXQ15StQslsaPwYrzqlw3u8AbD6ywOFBh+oq0HB76fCnX/XdOlet+sXAzG7ZXXbaIiIjUn0n48V0AACAASURBVBKvckIxgGpL6t8fK2/8nzj32R8I+p3PijYv5IW4x0mwYmJ2ri9bLjOviJRbJzDNrw37aPoiJsfdxKZlc0i5dQJz1+8AoCTgD2GxLY9/frmUYHDPiwqufnMOpz/5Xb1jFhERkaop8Qqxjlb/QVE3uGRmrdtR9jzuP0eVPXZR0axNGM3lhW+yYKO3jd+96g1JcVzUfPpGbeb3G24GYOryioO1ZuQU8sw3q5i+uvKrKLMLSuods4iIiFRNiVeIVT9QRfX6RG3hLzHvMNjW8HTskxXLDXjJ0cVF75VNK9qtM3038wZk/df/VpBy6wRWpGVzmC0liZ2cFTWNJeMfgmL19xIREQkXDScRYl8Fh5c9nhkcwGFRy2u97uFRSzk8ailjYz7eY15ebnbZ42temkIC0bwS+0/Y0osetuftiAD++PZc1ibct2tCLvDAC3BP/Wrlpq/axv5dEmnfKq5e64uIiOxrVONFaDrXLwimABWHmdjq2pc9PrrwX8wN9mNxsHe9yn/io+/LHn8edxv3xrzGyOjF8OIp3BL7Ttm8tuQQRzF/i3mDtQkX1WtblSkOBLnwhR+59OUZjVbm3mDZlmyca0g9p4iI7M2UeJXT0M71nwRGMq7kRAAuKrqDUYUPV5h/Z/HlZY9TXSfOKbqPXxXdX7ZOXbwY91jZ455R6ZwfM8V7UpJfYbmfE65iecIYfh/zec2F5m2nE16fsj2Sh/Rl8O0/wZ8eLMojmgBXpj8Ic/9b5/j3Rt8uT+e8f03k3Znra15YRET2SUq8gCP7dmiUcm4ovp7bS34PQBatWO56AnB90VjuLh7DDtrwQ2AQNxVdXbZOCTHcXnIFuS6+UWKoj41zv4Sf34ZH+jAj4Tq6so1+d3wOmeshx2u2dG9fBF//HXK95/EPd2dq/J/5VdT38PG1EKzYv2z51mxu+u+PlJTvd1aUB08eCks+rXVs23IKWb8tjMNblFRxFeqOdbBpXrWrbkpdw/yEq2g/77kQBCYiInsDJV5Ai7jad3Xr3KbuCdKnwSN5LeDd63F08d/4IHjsbksYowofqXO5jaX7x7+F8X8oez494XpWxV0I/zoIHu0Hy78kK8dLftxj+0Oe12m/h5UbzHXT3AplvvzG6zy24lQCjx/IjiXfwOxX4d1LYfsqeOdi2Di76oBKiso6/R/50Ncc+09/pP6SQu9vxST48o6adywYgFmveOXVxtbF8PdOsKyS2sF/HwzPH1ft6i2KvONyyLYJe85c8y1s/rn67TtXVqPIxjmQW/NguZWZu34HH8/bWK91RUQktNS5HnZ92dXCB9ccWeMte+pjI8lcXzSWo6MW8kbgZH4Z/RNXx9S+Ziikxv2Wtv5DcwF4pM+ey7xYrrm099E8lDMNgPjcjcS/c/aey7/gL9+qE4x+B755ENZPh6IcSN4f0pfC6HfpGtjIG3EPwT2j9yzjmJugIBOePgyO+yvMfRN6joBznoed673n3/4TZr4Ev30NWnWE6HiITfDW//E5r1bvgF/C53+FLfO96W9dAHdshW0roTgPuhy8a5vrf4ReR8CyL2DdNMjPhLlvwG2pRAe9BC+5cN2u5Zd/BRNu8uIBGP0u9B/l3S4hdxu0SoJpT0BhDnz3KAw8Ay4cBy+cAO16w7U/wpe3Qdeh3r4OuxwS2sLa7yDlGL+cDIhtCXEtITeDohfP4B9F13HW0FrcOzRzPXz7KAw+B7of6pVdlWAQ1v8AKUfXXG6pQDFExVR9ewjw3n9527zzU1tBf3DgqOhKtlkC0dV8tK36xnudxLWq/fbKy1gJHfvVb93q5G7zznFS39qvEwxCsBhiIldjHjKBYoiOjXQUIo3OmkNH4OHDh7tZs2aFrPzsWe+S+NmVnFz4CCtdjyqXO7JvEuOuPIIPZqdy03s11F40mOPy6C/4LHAEh0Ut49io+VwQM4UiF02c7RqRfqtrR2fLDHEse6F+p8DKSfVbt+fhsOGnPSZPH3ALI5c3Qs1lbCsozq16fvIBkL7Ee9yhr1eLCF4iunn+rv3qdzK07eHVNh7wK1jyCRxyCbRMgu//tWe5/Ud5SdDKSTDobIiOgwXvQmI3uOC/XjIIXiIVLIEzHoMhF8L8d+DAc2HjLG9bi/2rcPf/JSz9zDvW6cu85HPU32HnRvjp2V3b7XYobJoDPY+A1snQthecci/kbIVProffvOQlSRnLoeNAWPwRfHilt+4hF8Ppj8KWBd4XdbueXk3tQedB76Pgsz9B54Ng2BgvAS89bgCj34O+J8LK/8HEv8DODV6ym7kOjrwBhl8OrbvAD0/BlAe97Uz8i7fuUX/0mudPewjWToMhF8APT+5qRj/jMfjmH9CmG5xwu3dMBp/jHYeM5dBpkJfkphztLbNjHTw9zFu374kQkwDnPAcrJ3vlDzjV+9s0D1q0gxbtvfU/+D0s8IeUuS3Vq90187ZRUujtS/4OaNEBAkUw4UboOACume7V4mb5NaMjroIZz8PJ90KHPrD/mTD/be+HRdch0LE/LJ0AG2ZAfGtI6g8JbSB7i1de3xO9HzQb58DUR+CCcRAV5SWG6UtgZyq06+V1N+gxDBZ95P0guXKyd8zNvHlrp8Hyz73Ees5r3jE76DzvvLfrBTH+FdSb5oELwqIP4Zi/wML3ISoWeo30Ys3aBLlp0O0Q7zVdmO29nlp38t5fwRIv3rztYFEQnwjfPQbDf+cdK/De4+17e+e5fW/osJ/3/irOh0l3wu++gO7+Ods42zsmLgAJ7bz9CZTAu5d4561DX+812ml/6DTYiyN7i7cP0bHe81KZG2DFV95rqvTHwY513rQf/w+umuLtg1nFHx5Zm+ClUXDJRxDbAmb8Bw670jtP0XHeNPDeJ/PfgYMvqPgDpWAnxLTYdYxLirwfwisnez8uz3raO8alx2bg6d6yzx3j/ZA5+HxI6gcty3XdSZ3tHdsW7WHiTfCrp7z3QmJX6H+K15Vly3zocZi3/M9ve++70oR76QSwaBh4Gmxf4702SzlX8Qedc96P2H4nw4610OcYiG/rvQ7Ba0VZ8ZV3Ti7/HHofSSiZ2Wzn3PBK5ynxgpzZ79L605oTr98d1Ye7zhzEFws3c/WbVd92JxRiKaGbZbDOdeGG6A+5MfZ9APYveIWLoifzbfBgslxLJsf/hdZWwKPF5/F04BzOippGKytkfOAoliT8rkKZ5xfeyTvx94d1P0RERCLqt2/AoF+FdBPVJV5qagTi570KQAuKaJMQQ1YNI7f3aN8yDFFVVEwM61wXAJ4KnM24wElk0ooSYngpcHrZcgcWvlxhvY+Du5qFLiu6mTbkMSk4jFhKyKI1lxb9lT62hQzXlrExH3F98VjWui6sTKi8merMwr/zafzfqoxzXbATvaPSGrKrIiIioRPhpnklXlBWrZlkWVxx1oH86Z3qr147sHs1/WDCwBFFBnWPYUrwkLLHpYNOfBscwrcMAWBC0RFl81MKxtGFbUQTJI32rEi4lEuL/soCtx8pBeNoQy7Jlkk8xSx2KdVutxsZ5JJADi2Io5gWFLGdNmXzn/rNQM6In8vR7zvyioL8MeZDjvVve3RV0Z/5dfQ0trk2LHG9GBc4iW6WQarrRHfS+WX0j7SwQv7Y6Wdu3DKKycFDyKI1UQS5Pno8MRYghgBPlZxDKwp4Lu4J0odez6j+rYk64JcEF33M2/MzOWLFY+wXtYVXS0bxYuB0JifeS5QZC3teyCErn+Hu4jF8HhjBqR0zuOfgTKLjW8E3f2dnXFfaFm0u25eAM6LNq0UOxLZmesdzuW7NkSRbJv+7op/XhPLzOFg0HoCpBz7IcQtvp7DDAcQfccWupiyAPsd6zSwFO2HrIljzHWSl7nF87y2+hJWuO290Gw8Zy7i46DbejPtH2fx5wb4cOGQEMSfe5jUvAZz2sNdEOeN57/mhY7ymnVLDfwcFWbBjjdeEMv9tANYGO5PSBq/ZZne/eho2z/MuIkidWTZ5WmAwR0cv8p7EtNhjyBM6DYK0xWxJPpIu6T/4+34crJm65zZKte3pNQ0CtO4MJ98Dk+72mpcq20Z0nNfUFh0Hx99G0bKviEudXnX5VWnZEfIqv+jhIzuJ7GLjvIPakbDkfe/cgde80bqz1/wXIi93voPfbX0gZOVXqU13r5lst4trGqTjQMhYVvm82JZev8t9Xfdh1V+gVF/luy6EU+vOlX+mhEJcovf5EkFqagSy1y9k8YtXcHnRLbzw++O46MU9++/ArqZGgJ15xQy576uQxSShdf/ZB9I3uRWjX6j8XBtBonAEiGbtQ2eQcuuuKxWvO6EvbVvEkl1QwlNfrwTgxdEHMm/FWp6ZmcWJUXPJadmDn3I6Vyhz5H5JvHWVl9we+Y/JbNq563ZNt5w2kFGDOtOvUyIAT01eQVEgSHZBCTvzi3ni/KF8+vMmrn/L+4Lr07EVazJy+c8lw/jDG94H8CuXH8blr3gJTyvyWXTH0RzwyDzyiwMsuvdUWsV7v7MWb8qisCRATFQUH85N5a5fDsJK+0oUFzB+YQYr03JYm5HHP35zENtzikhqHcdB93iv97UPneEtW5Dl9R+pxJadBSS1jiM2Oqrs2K196AzSsgrILQrQp6PXdyUQdHy/MoN7P13EqvRcFtwyjMT8jbv6zuRnev1Y4hP33Miyz71+P50HVxoD4PVzioqp0Bdm8aYsTn/yO/7x64O4cEQvduYV839TV/KXUQOJTVvg9Tlq0c7rM5Kb7iW+xfle36HSfjKVGHb/JLblFjHjjpPolJhQyUFZ4CWZu18QECiB4jzSi+NJdDnEt2qLs2iioiq5IKEo1+uvtHGO10cmUEzKXd+UHV+AWWu38/L3a3j6wkPLytiUmc+RD33N+1ePZHhKDcPn5Gd6CfmgsyB5oLe9uNb+RUjO6xNVnO9dzFHp+ju8vjVFObteH5vn8/dPfqb3wUdzychy/XRWT/HK63Os15euxwjodXj18VUlbalXVvKAPec5BzlpkNh5z3m1VZjjvZZiy53bolyv/1pS/119iUq/U+e+CQf+es+LOIpyvddlaV+o4vxqX1fkZkD2ZuhyUPXx5e/w+n7FxMGGmbv6drXr5W2vqvPlnNfPqvR1X5lAifdjJn2510cPvHPXa+Su2qNg0OvjVv6CCOe8/Ssp8Pp5ObfrOO2uyO/XWnq8du/DlZ8JyyZ6/U9z07x93bl+12dFecGA94Nn01w44hpvWReseO5CTH28ahAMOk7797cc0LUN/77gkApfsuWVT7wAlm7JIhiEM576ri4XRkozM++uUxh6Xz074u/mwhE9eWvGhirnL77vVEqCjoPvqZjUt20Ry8784mrLbtcylsy8XctUSMouO4zLX53JUxceUpa8lZp75ym8OG01Y0/oz5lPT2NlWk7ZvNt+sT//+HxpheX/8euDSIiN4pxDvP6QgaBja1YBDujWNoGcwhIOuucr2iTEMOGGYzjmES8xeOeqIzj/+R+BXUlCVe+1suTOl5FTyPrtebRvGUer+GhKAo5u7bwvqyWbsxjYOZHJS9N4adpqnrt4GO1a7nkbq/Xb8oiPjeL7lRnc+O7PZdu59YP5vD1zA4+dN4SzhnZj8N1fEmXG4vtOLUtIF27cyR0fLeTnDZk8cf4QzjmkByu2ZtOvU2vMjJzCEg68+0sAbj51INedUPlVj7PX7WBlWjY92rfkqH4Vr+JMuXUCw3q359Be7XjhuzWsftDrQvDk1yu4dGQKLeOimZ+6kxF9OuyxXvljNvS+r8jMK2bunafQvlUczjn++PY8Pvl5E2bQMjaaeXePIja65tGErnlzNr8d3pMT9u9UYfrWrAKSW8dXSA7Tswu59OUZ7Mwr4ofbTmJlWja9OrQiLiaq0jirsjWrgE6J8bt+DOxmwvzNjOybRPuWsZz9zPf84bi+nH5Q1xr3pb5mr9tOzw4tKyTTy7ZkM6Bz6ypjvPfTRQzonMiFI3pREgiSXxwgMaHpX6FZUBwgLjqqwnl1zpFbFKB1fOM1kH00dyMj+nQoew+XbmdlWg5mlP0AfePHdaQkteSY/skV1p+4YDNH9e1I25beMf1maRpz1+/gxlEDGy3GhlLiVUdfLNxMQmw0t7w/n7TsXQNq7p54lfp+ZUaVtWQizcVxA5KZurzy+3xW5v6zD+Thz5eSU7irT+RlR6bw6g9ra1z3jyf158IRvTjiH5MrnT/+2iO58+OFLNyYVWUZJx/Qif8tqbw/YZTBgM6JdGqTwNAebXnSr5nc3WEp7Zm5dkel89684nDyiwPc+sF8tuXuGguuZVw0pw3uwodzvSsC7z5zEPd+urjKOME7LjeOGlAhoX74Nwfx1w8WVLnOfy4ZRsu4aC55ac/bcj0z+lCuGzeHcb8/nNH+Z88NJ/bjzZ/Ws71crO9fPZK56zN5YOKSCuv/fNcovli0mb9+sIAnzh/C4X2S+M/UVSQnxnNor/aM7JtEIOi8gZR9M24/iaJAkNvHL+Rb/3Wy9P7TeGvGen41pBvD/v6/XcvecRIjHvDO7c2nDmTaigymr95WVs6WrAKyC0rom9yaZ6esxMz42xkHcOfHC3lrxgY6t4ln7In9OW9Yj7IE0YD0nEIOf9Ar97tbTihL6p+7eBjDercnvyhAr6SWLN2SxWn/+o53/zCSgZ0T+XBuKpcdmcKzU1fx8rQ1FBQH+f0xfbji6D4kJsQyd/0OenZoydasAgZ2TmTKsnSmLk9n7In9OPzByXRsHc8rlx1Gn+RW/Lwhk4te/IkLDuvpbWtzNnlFAR76zUF0bO3V/pRPMq95czafL9zC478dwq8P3XXxVjDouOzVmXy7PJ01/zid71Zk8Nik5Twz+hACQe+HRW5hCWPHzWXaygy6tU1gwg3HUBQI0inR205OYQn/W7KVMw/uxqNfLWfehh28fdXICud64oLNLN+azYDOiZx+UFd+Wr2NnfnFjBrs9RnOzCvi2SmruHHUAAb+7QtOPqAzPdq34C+nDuSUx6eSU1BCdmFJWSI/b0MmvTq0pIN/j94TH5vCaYO7cP2J/Xnmm5WMPbEfCbG7anXfnrGeLVkF/OlkryYyvyjAAXd9UXZ8wEu6Ln91JlOWea+rv51xAKMGdSkbw3HtQ2fgnMPM2LA9j2Me+YYTBibzyuUjKhzveXedQkJsNDmFJXw4J5UHJy5lwT2jaBUXU3kNcggp8aqn3z43nRlrt5c9r+qXrHOOPrdNDGdoIiLSCM4f3pN3Zu2qhU5JasnacN4tI0T269iKg3u05aN5m0K2jdtP358HJy6teUEgMSGG7CouXKvporaxJ/Tj6W8q//E0vHd7Zq2r/MdTVVY9eDrRIU7ElHjV047cImav28HvX/e2vfKBXxBTRfX8vA2ZnP3M95XOExERkabhn+cezHnDe4Z0G9UlXrplUDXat4rj5EFeZ8yUpJZVJl0AQ3vu6pT44bW7BmaLCXP1poiIiFQtJjqy38saTqIWvvnL8WXt2bVxaK/2FdYt7Yewr4iPiaKwJFjzguUcPzCZlKRWteofFG7H9O/Idyt2DSHw4qXDOXlQZzZm5rM2I5dJi7dy4YhePDl5BRMWbK6mpF3+df7QPYYtOXH/Tjz064P4OXUnV75eeQ3vmJG96Z3Uitho486PF1WY99h5Q/jnl8vYkuVdLTn15uM57p9TAIiLjqIoEOTa4/vy7Yp0Fm7M4s8nD+CJ/3lDHIzo04Fxvz+cLVkFvD87lVXpuUQZHJbSgTd/XMcNJ/Xn2v96gwbfctpAfnFgV1J35DFjzXb+cFxf5m/I5P4JS4iLieLXh3QnITaK71ZkcO6wHiQmeB8zLWJjGNStDavTc+jWrgXRUUZ/vw/RLw7swur0XAZ0SeTCET0Z0qMd783awD2fLi47Vu9fPZIBXRJpFRfDuBnref2HtXRt14I7Tj+AH1dv4+AebcnMK6Zfp9b0aN+ClWk55BUF+PTnTWTkFDK0ZzvemZXKH0/qR68OrSgKBGnXIpYP527k2Skr+d+NxxFlxrX/ncOoQZ05sl8S36/cRsu4aJzzLnD4Zlkany/cwnMXD+PqN2dz3Ql9ueb4fpQEgjw+aTmvT1/HC5cOp22LWL5emsZzU1fx6HlDWJWew5kHd2P2+h0c3qcDndskkBgfw5/emcclI3vz3JRVFAcd5xzSjTUZeRw3oCMdW8fz/uxUZq7dTqfEBG45bSDfLEtn5H5JtGkRQ+qOfB79chkbM/MZuV8SXyzawskHdObofh15a8Z6VqTl8L8bj+P+zxbz1aIt3HXmIF78bg0r0nK48ZQBFBQHyCks4c8nD2DDjjw+mJ3KqMFdOKRXu7KLNAJBR+qOfIoDQbZkFTByv6Syz7PzhvXg9IO7klNQwhH7JZGRU0jnNgkEgo7MvCJio6OYsXY7t7w/n3l3nUJsdBS5RSUkxsfy4+ptfLsine7tWtCjfUsOS2lPWnYhF734Ex1axRF0juMGJHP8wE6kZxcyrHd7Nu/Mp0/HVnw8bxNLNmexKTOfvsmtue6EfqxIy2bKsnSuPGY/enZoyYbtedz8/s8Eg3DzaQMZ2rMdBvxvyVY+nb+ZzZn5PHfJMBZtzKIk6Dh+YDLFgSALUnd65yYhhpVpOazfnkdCbDSf/ryJG07qT0ZOITmFJQzu1pZoM9KyC9iaVUh+cYBTBnWmsCTAnHWZ/OPzJYwa1JnzD+vF+u25vPbDOvp0bMWpg7uwKj2HA7u3JSO7kIycQrq3b0H7lnG0SYhl8858Ji7YjJnRo30L0nMKOXtod64bN4frT+zHR3M3ceyAZK44es/bti3cuJOlW7I5dkBHWsbF0DI2mtyiEtZvz2NwN2/4oY2Z+fz3x3WMn7uRN64YweLN2SzbksU5h3QnJakVn83fTO+kluQUlnD7+AVs2VnAXb8cRPf2LUjLKuTLRVvILijhjjMOYN6GTLq1a0Hr+BhaxkXzxaIt9ElqxUE92pLUKp6lW7JISWpFfnGAhRt3smFHPtef2I9oM6av3kb7lnG0jo8hPaeA3zw7nZtPHchbM9bz0K8P5qEvlvDPc4cwfu5GMrILufr4viS3jueDOaks25INwOkHd8WADTvyOX94T4oDQf7wxmyOG5DM2zPXc90J/YiLiWJIj3bc++kirjuhH+u353H/Z0voXNlVx2GkpsZG9NoPa+nfqTVH9uvov4G2cMXRfbj05Rl0bB3H3WcOZsi9Fa9WO7hHWw5L6cBL09bUaVs3nNSfJyevaLTYrzi6zx4xLLhnFJl5xSQnxnPPJ4t4e6bXD6Jb24QKQyEkJsQw7a8nsnDjTo7q15HCkgA/rt7OmJdn8OxFh3LKoM785tkfaN8qjinL0jltcBd+M6wHs9ft4KLDe9GzQ8XLnL9YuIXFm3aWXaFSHAhy+SszSenYkgWpO3nukmG0bxlHQmw0O/OLSd2Rx8DOiTw7ZRW/O7oPazJyKSwJMKSHd3XY745OIT5mz3v6FRQHcG7XFXMpHVtx7rM/8MYVI8quqknLLiCpVTybMvNZtCmLnMISzh1W+d0NCooD7H/nF4y78nCO7FvxirUduUXERBsxUVG0iPNiySooJr8oQOc2e34IrNuWS1Lr+BqvJCoJBCkJugqdWYsDQfIKA7RtGUtJIMiajFz6d65kOAagqCTI7HU7GNk3qdrtlCrt4NpYgkGHGY1aZqgVB4K1uiKwKQoEHUu3ZJV9EddHczxnIuHW5Pp4mdlpwL+BaOBF59xD1S3fXBKvUMstLCE6yip8yZaev90/BAuKA8RGR5V1INyRW0S7lrFly2XmFZGYEFvrDoYlgSC5/pe5iIiIVK1J3TLIzKKBZ4BTgFRgppl94pyr/npsKRsAs7yqfnWWT87A669WXmXjHFUnJjqKti2b5698ERGRpiIS36QjgJXOudXOuSLgbeCsCMQhIiIiElaRSLy6A+WH7k71p4mIiIjs1SKReFXWNrZHRzMzu8rMZpnZrPT02o+mLSIiItJURSLxSgXKj1zWA9hjaF3n3PPOueHOueHJycm7zxYRERFpdiKReM0E+ptZHzOLAy4APolAHCIiIiJhFfarGp1zJWY2FvgSbziJl51zi2pYTURERKTZi8jI9c65iYDuKi0iIiL7FA3MJCIiIhImSrxEREREwkSJl4iIiEiYNIubZJtZOrAuxJvpCGSEeBtSdzovTY/OSdOk89L06Jw0TeE4L72dc5WOhdUsEq9wMLNZVd3QUiJH56Xp0TlpmnRemh6dk6Yp0udFTY0iIiIiYaLES0RERCRMlHjt8nykA5BK6bw0PTonTZPOS9Ojc9I0RfS8qI+XiIiISJioxktEREQkTJR4AWZ2mpktM7OVZnZrpOPZm5nZy2aWZmYLy03rYGaTzGyF/7+9P93M7En/vMw3s0PLrTPGX36FmY2JxL7sLcysp5l9Y2ZLzGyRmf3Rn67zEkFmlmBmM8zsZ/+83OtP72NmP/nH+B0zi/Onx/vPV/rzU8qVdZs/fZmZnRqZPdp7mFm0mc01s8/85zonEWZma81sgZnNM7NZ/rSm+RnmnNun//Bu1L0K2A+IA34GBkU6rr31DzgWOBRYWG7aI8Ct/uNbgYf9x6cDnwMGHAH85E/vAKz2/7f3H7eP9L411z+gK3Co/zgRWA4M0nmJ+HkxoLX/OBb4yT/e7wIX+NOfA67xH18LPOc/vgB4x388yP9ciwf6+J930ZHev+b8B9wIjAM+85/rnET+nKwFOu42rUl+hqnGC0YAK51zq51zRcDbwFkRjmmv5Zz7Fti+2+SzgNf8x68BZ5eb/rrz/Ai0M7OuwKnAJOfcdufcDmAScNr/t3d3oVZUYRjH/w9pKipa9nGRgQmGUaRChaKFmAiVRIlgJBQZ9AEVCiGW0LVgRF10EwbdmEHahxeRRWmCUYqmx68sw6DQNChNiyTt7WK924bTPqJ2zuy9j88PhtmzZvbsmXlh5t1r9XcWDgAABPlJREFU1szq+63vnyLiUERsy8/Hgb3ANTguLZXH90RODswhgBnA6izvHpdGvFYDd0pSlr8VEScj4gCwn3LeswsgaTRwD7Aip4Vj0q7a8hzmxKtcYH6oTP+YZVafqyPiEJQkALgqy3uKjWPWR/JWyCRK7Yrj0mJ5S2s7cIRyEfgOOBoRp3KR6jE+c/xz/jFgFI5Lb3sZWAz8ndOjcEzaQQAfSdoq6bEsa8tz2IDeXmEHUpMyP+rZHnqKjWPWByQNA9YACyPit/LHvPmiTcoclz4QEaeBiZJGAu8CNzRbLMeOSx+TNBs4EhFbJU1vFDdZ1DGp39SIOCjpKuBjSV+fZdmWxsU1XiWjvbYyPRo42KJtuVgdzmpecnwky3uKjWPWyyQNpCRdKyPinSx2XNpERBwFNlDao4yU1PjTXD3GZ45/zh9Bua3vuPSeqcC9kr6nNEuZQakBc0xaLCIO5vgI5U/KbbTpOcyJF2wBxuVTKZdSGkCubfE2XWzWAo2nRx4G3q+UP5RPoEwGjmV18TpglqTL8imVWVlmFyDbnLwO7I2IlyqzHJcWknRl1nQhaQgwk9L+bj0wNxfrHpdGvOYCn0ZpMbwWeCCfsLsOGAdsrmcv+peIeC4iRkfEGMq14tOImI9j0lKShkoa3vhMOffsol3PYa1+EqEdBsoTDt9Q2k8sbfX29OcBWAUcAv6i/Lt4lNLm4RPg2xxfnssKeDXjshO4pbKeBZQGqfuBR1q9X508ANMo1eldwPYc7nZcWh6Xm4GvMi67gBeyfCzlIr0feBsYlOWDc3p/zh9bWdfSjNc+4K5W71t/GIDp/PtUo2PS2liMpTwlugPY3biOt+s5zG+uNzMzM6uJbzWamZmZ1cSJl5mZmVlNnHiZmZmZ1cSJl5mZmVlNnHiZmZmZ1cSJl5m1NUmf53iMpAd7ed3PN/stM7O+4tdJmFlHyC5ano2I2efxnUuidLvT0/wTETGsN7bPzOxcuMbLzNqapBP5cRlwu6TtkhZlB9LLJW2R1CXp8Vx+uqT1kt6kvBwRSe9l57m7Gx3oSloGDMn1raz+Vr7RermkXZJ2SppXWfcGSaslfS1pZb75H0nLJO3JbXmxzmNkZp3DnWSbWadYQqXGKxOoYxFxq6RBwCZJH+WytwE3RcSBnF4QEb9k1ztbJK2JiCWSnoqIiU1+aw4wEZgAXJHf2ZjzJgE3Uvpw2wRMlbQHuB8YHxHR6OrHzKw713iZWaeaRelvbTvwJaV7kHE5b3Ml6QJ4RtIO4AtKJ7jjOLtpwKqIOB0Rh4HPgFsr6/4xIv6mdK80BvgN+BNYIWkO8Mf/3jsz65eceJlZpxLwdERMzOG6iGjUeP1+ZqHSNmwmMCUiJlD6Pxx8DuvuycnK59PAgIg4RallWwPcB3x4XntiZhcNJ15m1imOA8Mr0+uAJyUNBJB0vaShTb43Avg1Iv6QNB6YXJn3V+P73WwE5mU7siuBOyidHDclaRgwIiI+ABZSblOamf2H23iZWafoAk7lLcM3gFcot/m2ZQP3nym1Td19CDwhqQvYR7nd2PAa0CVpW0TMr5S/C0wBdgABLI6InzJxa2Y48L6kwZTaskUXtotm1t/5dRJmZmZmNfGtRjMzM7OaOPEyMzMzq4kTLzMzM7OaOPEyMzMzq4kTLzMzM7OaOPEyMzMzq4kTLzMzM7OaOPEyMzMzq8k//hd560FciBwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(W, new_word, k=5):\n",
    "    x = src_embeddings[src_word2id[new_word]] # embedding on of new_word in the source space\n",
    "    z = np.dot(W, x) # embedding of the translated word in the target space\n",
    "\n",
    "    # representation closest to z in the target language space, using cosine similarity as the distance metric\n",
    "    z_pred1 = np.argmax(cosine_similarity(z.reshape(1,300), tgt_embeddings))\n",
    "\n",
    "    # top k closest word embeddings in the target space\n",
    "    z_predk = cosine_similarity(z.reshape(1,300), tgt_embeddings)[0].argsort()[-k:][::-1]\n",
    "    \n",
    "    return [tgt_id2word[z_pred1]], [tgt_id2word[z_predk[i]] for i in range(len(z_predk))]  # return the id of the translated word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0372,  0.0441,  0.0013],\n",
      "        [-0.0288,  0.0582, -0.0241],\n",
      "        [-0.0095, -0.1309,  0.0246]])\n"
     ]
    }
   ],
   "source": [
    "W_trained_test = gener.l1.weight.data\n",
    "print(W_trained_test[:3,:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 translation for 'bateau' is : ['ever']\n",
      "Top 5 translations for 'bateau' are : ['ever', 'champions', 'technologically', 'championship', 'materially']\n"
     ]
    }
   ],
   "source": [
    "# just a litte test\n",
    "top1, top5 = prediction(W_trained_test, \"bateau\", k=5)\n",
    "print(\"Top 1 translation for 'bateau' is :\", top1)\n",
    "print(\"Top 5 translations for 'bateau' are :\", top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ever', 'champions', 'technologically', 'championship', 'materially']\n",
      "--\n",
      "['loggins', 'brecker', 'flanagan', 'kelly', 'brubeck']\n",
      "--\n",
      "['methodist', 'promptly', 'carl', 'methodism', 'likewise']\n",
      "--\n",
      "['annoy', 'so', 'piss', 'hurting', 'therefore']\n",
      "--\n",
      "['glad', 'regret', 'adamant', 'commented', 'terrific']\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "src_words = ['bateau', 'maison', 'argent', 'ordinateur', 'dieu']\n",
    "tgt_words = []\n",
    "for i in range(0,len(src_words)):\n",
    "    top1, top5 = prediction(W_trained_test.numpy(), src_words[i], k=5) #top 1: english word (target)\n",
    "    tgt_words.append(top1[0])\n",
    "    print(top5)\n",
    "    print('--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 possibles functions of accuracy = \n",
    "# - Test if we traduced well the word = sum Indicatrice(Wxi!=zi) (en gros nb de mots mal traduits) # en fait non c'est con on a pas les traductions\n",
    "# - Test how far we are from the \"supposed\" translation\n",
    "# compute Wxi find the closest z (SUPPOSED TO BE ITS TRANSLATION) and cumpute eculidian distance\n",
    "def test_accuracy(X_text,W_trained):\n",
    "    loss = 0\n",
    "    for x in X_test : # get all french words\n",
    "        word2id = {v: k for k, v in src_id2word.items()}\n",
    "        word_emb_new = np.dot(W_trained, x)\n",
    "        scores = (tgt_embeddings / np.linalg.norm(tgt_embeddings, 2, 1)[:, None]).dot(word_emb_new / np.linalg.norm(word_emb_new))\n",
    "        best = scores.argsort()[-1:][::-1]\n",
    "        nearest_eng_emb = src_embeddings[best]\n",
    "        loss = loss + np.linalg.norm(word_emb_new-nearest_eng_emb)  \n",
    "    return(loss)\n",
    "    \n",
    "# Pous savoir si le model est pertinent tester l'accuracy pour W_trained a epoch 1,5,10                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_test_word_id = torch.Tensor(30).random_(nmax).long()\n",
    "X_test = src_embeddings[rand_test_word_id.numpy()] # GARDER LES MEMES POUR LES TESTS\n",
    "\n",
    "test_accuracy(X_test,W_trained_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a translation French to English dictionary\n",
    "def prediction_dict(dico, W, k):\n",
    "    dico_pred1 = {}\n",
    "    dico_predk = {}\n",
    "    i = 0\n",
    "    for word in dico.keys() :\n",
    "        if (i%100==0):\n",
    "            print(\"Progress :\", round(i/len(dico_test)*100,1), \"%\")\n",
    "        dico_pred1[word], dico_predk[word] = prediction(W, word, k) # lists\n",
    "        i += 1\n",
    "    print(\"Done...\")\n",
    "    return dico_pred1, dico_predk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-top accuracy (\"acc@k\")\n",
    "k = 5\n",
    "\n",
    "dico_pred1, dico_predk = prediction_dict(dico_test, W_trained_test, k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure of the accuracy of the dictionnary\n",
    "# output is a list\n",
    "def accuracy(dpred1, dpredk, dico):\n",
    "    acc1 = [0]\n",
    "    acck = [0]\n",
    "    \n",
    "    for key in dico.keys():\n",
    "        add1, addk = 0, 0\n",
    "        \n",
    "        if dico[key] == dpred1[key][0]:\n",
    "            add1 = 1\n",
    "        acc1.append(acc1[-1] + add1)  \n",
    "    \n",
    "        for i in np.arange(k):\n",
    "            if dico[key] == dpredk[key][i]:\n",
    "                addk = 1\n",
    "                break   \n",
    "        acck.append(acck[-1] + addk) \n",
    "    \n",
    "    acc1 = [i/len(dico) for i in acc1]   \n",
    "    acck = [i/len(dico) for i in acck]\n",
    "    \n",
    "    return acc1, acck # nb de mots bien prédits/nb de mots total     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1_test, acck_test = accuracy(dico_pred1, dico_predk, dico_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc1_test, label=\"Acc_1\")\n",
    "plt.plot(acck_test,label=\"Acc_k\")\n",
    "plt.plot([i/len(dico_test) for i in range(len(dico_test))],label=\"identity\",alpha=0.4)\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Iteration')\n",
    "plt.title(\"Accuracy test set (Gradient Descent method)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Gradient descent method :\")\n",
    "print(\"Final accuracy @1 =\", round(acc1_test[-1]*100, 2), \"%\")\n",
    "print(\"Final accuracy @5 =\", round(acck_test[-1]*100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to pair words\n",
    "\n",
    "CSLS tentative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5 # hyper parameter K-NN\n",
    "CSLS = np.zeros(nmax)\n",
    "\n",
    "def CSLS_measure(W, new_word, k):\n",
    "    x = src_embeddings[src_word2id[new_word]] # embeddings of new_word in the source space\n",
    "    z = np.dot(W, x).reshape(-1,300) # embeddings of the translated word in the target space\n",
    "    \n",
    "    knn = NearestNeighbors(n_neighbors=k, algorithm='auto')\n",
    "    for t in range(tgt_embeddings.shape[0]):\n",
    "        dist_tgt_nn_z, index_tgt_nn_z = knn.kneighbors(z)\n",
    "        emb_tgt_nn_z = tgt_embeddings[index_tgt_nn_z[0][:]].reshape(-1,300) # embeddings of the k-nn in the tgt space\n",
    "        r_T = (1/k)*sum(cosine_similarity(z, emb_tgt_nn_z))\n",
    "        \n",
    "        dist_tgt_nn_yt, index_tgt_nn_yt = knn.kneighbors(tgt_embeddings[t])\n",
    "        emb_tgt_nn_yt = tgt_embeddings[index_tgt_nn_yt[0][:]].reshape(-1,300) \n",
    "        r_S = (1/k)*sum(cosine_similarity(tgt_embeddings[t], emb_tgt_nn_yt))\n",
    "        \n",
    "        CSLS[t] = 2*cosine_similarity(z, tgt_embeddings[t]) - r_T - r_S\n",
    "    CSLS = np.argmax(CSLS)\n",
    "        \n",
    "    return CSLS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
